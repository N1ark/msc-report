\chapter{Background} \label{chap:background}

In this chapter we introduce some of the core ideas used throughout this document, while mentioning pre-existing work in the field. In \cref{sec:separationlogic} we introduce Separation Logic, its variations, and the idea of Separation Algebras; in \cref{sec:gillianverif} we introduce Gillian, a CSE engine; in \cref{sec:existing-tools} we mention other similar verification tools; finally, in \cref{sec:legal-requirements} we evoke the legal, social, ethical and professional impacts of this work.

\section{Separation Logic} \label{sec:separationlogic}

As the need to formally verify programs grew, methods needed to be built to provide a framework to do so. Hoare Logic \cite{hoarelogic} allows proving properties of programs, by describing them axiomatically. Every statement can be expressed as having a precondition -- the state before execution -- and a postcondition -- the state after \emph{successful} execution -- expressed as \SLtriple PSQ. For assignment for instance, one could have \SLtriple{\code{x}=x}{\code{x} := 0}{\code{x}=0}.

While already helpful to reason about programs, an issue remains: shared mutable state. How does one describe that there is a list in memory of unknown length, that may be mutated at multiple places in the program, while ensuring properties hold at a specific point in time? Being able to describe the state and constraints upheld by global state is difficult, and past solutions scaled poorly.

Separation Logic (SL) \cite{seplogic1, seplogic2} is an extension of Hoare logic that permits this in a clear and scalable way. Its main addition is the separating conjunction $*$: $P * Q$ means not only that the heap satisfies $P$ and $Q$, but that it can be split into two \emph{disjoint} parts, such that one satisfies $P$ and the other $Q$. This allows us to reason compositionally about the state, by not only stating what properties are upheld, but how they may be split for further proofs. For instance, given a \emph{list} predicate, when calling a function that mutates the list one can simply substitute the part of the state corresponding to that list with the postcondition of the function, with the guarantee that the rest of the state is untouched, by using the \emph{frame~rule}:
\[
\inferrule[Frame]
	{\SLtriple{P}{S}{Q}}
	{\SLtriple{P*F}{S}{Q*F}}
\]

Because we know that $S$ only uses (either by reading or modifying) $P$, any disjoint frame $F$ can be added to the state without altering the execution of $S$. This is very powerful: one can prove properties of smaller parts of code (like a function, or a loop), and these properties will be able to be carried to a different context that may have a more complex state. For instance, this can be used in a Continuous Integration (CI) setting, by only analysing functions of which code is modified, while reusing past analyses of unchanged code, allowing for incremental analysis.

SL also comes equipped with the \emp{} predicate, representing an empty state (i.e. ${P * \emp = P}$), and the separating implication $\wand$. $P_0 \wand P_1$ states that if the current state is extended with a disjoint part satisfying $P_0$, then $P_1$ holds in the extended heap \cite{seplogic2}.

Further predicates can then be defined; for instance the ``points to'' predicate, $a \mapsto x$, stating that the address $a$ stores values $x$. For instance, $a \mapsto x * a \mapsto x$ does not hold, since we can't have two disjoint states both containing the address $a$. Similarly, $a\mapsto x * b \mapsto y$ only holds if $a\neq b$.

\subsection{Incorrectness Separation Logic}

Separation Logic is, by definition, \emph{over-approximate} (OX): \SLtriple PSQ means that given a precondition $P$, we are guaranteed to reach a state that satisfies $Q$. In other words, $Q$ may encompass \emph{more} than only the states reachable from $P$. This can become an issue when used for real-world code, where errors that can't occur are detected and flag the code as incorrect, hindering the use of a verification tool (for instance in a CI setting).

To solve this problem, a recent innovation in the field is Incorrectness Separation Logic (ISL) \cite{isl}, derived from Incorrectness Logic \cite{incorrectnesslogic}. It is the \emph{under-approximate} (UX) equivalent to SL, instead ensuring that the detected errors actually exist, at the cost of possibly missing some executions of the code.

Where SL uses triplets of the form \SLtriple{\text{precondition}}{S}{\text{postcondition}}, incorrectness separation logic uses \ISLtriple{\text{presumption}}{S}{\text{result}}, with the result being an under-approximation of the actual result of the code. The reasoning is thus flipped, where we start from a stronger assertion at the end of the function, and then step back and broaden our assumptions, until reaching the initial presumption. This means that all paths explored this way are guaranteed to exist, but may not encompass all possible paths. This enables us to do \emph{true bug-finding}. 

Another way of comparing SL to ISL is with consequence: in SL, the precondition implies the postcondition, whereas with ISL the result implies the presumption. Furthermore, ISL triplets are extended with an \emph{outcome} $\epsilon$, resulting in \ISLtriple{P}{S}{\epsilon:Q}, where $\epsilon$ is the outcome of the function, for instance \Ok, or an error.

While SL is over-approximate and ISL is under-approximate, we may call \emph{exact} (EX) specifications that are both OX-sound and UX-sound \cite{exactsl}. Such triplets are then written \ESLtriple{P}{S}{o:Q}, with $o$ the outcome, and $P$ and $Q$ the pre and postcondition respectively.

\subsection{Separation Algebras}
\label{subsec:separation-algebras}

While the above examples of separation logic use a simple \emph{abstract heap} mapping locations to values, we often need more complex state representations to verify real-world programming languages. For instance, the model used by the C language (in particular CompCert C \cite{compcert}, a verified C compiler) uses the notion of memory blocks and offsets: memory isn't just an assortment of different atomic cells. Furthermore, the basic ``points to'' predicate, while useful, has limited applications; for contexts such as concurrency, one needs to have a more precise level of sharing that goes beyond the \emph{exclusive ownership} of ``points to''.

An example of such extension is fractional permissions \cite{fracpermissions, fracpermissions2}: the ``points to'' predicate is extended with a \emph{permission}, a fraction $q$ in the $(0;1]$ range, written $a \mapstoq{q} x$. A permission of $1$ gives read and write permission, while anything lower only gives read permission. This allows one to split permissions, for instance $a \mapstoq{1} x$ is equivalent to $a\mapstoq{0.5}x * a\mapstoq{0.5}$, a program can thus concurrently execute two routines that read the same part of the state, while remaining sound -- permissions can then be re-added as the routines exit, regaining full permissions over the cell.

Further changes and improvements to separation logic have been made, usually with the aim of adapting a particular language feature or mechanism that couldn't be expressed otherwise. \citetitle{next700seplogics} \cite{next700seplogics} argues that indeed too many subtly different separation logics are being created to accommodate specific challenges, which in turn require new soundness proofs that aren't compatible with each other \cite{sljungle}. To tackle this, research has emerged around the idea of providing one sound metatheory, that would provide the tools necessary to building more complex abstractions \emph{within} that logic, rather than in parallel to it.

A first step in this quest towards a single core logic is the definition of an abstraction over the state. The main concept introduced for this is that of \emph{Separation Algebras} \cite{abstractseplogic, sepalgebra}, which allow for the \emph{construction} of complex separation algebras from simpler elements. Because soundness is proven for these smaller elements, constructions made using them also carry this soundness, and alleviate users of complicated proofs.

Different authors used different definitions and axioms to define separation algebras. In \cite{abstractseplogic} they are initially defined as a \emph{cancellative} Partially Commutative Monoid (PCM) $(\Sigma, \cdot, 0)$. This definition is however too strong: for instance, the cancellativity property implies $\st\cdot\st'=\st \Rarr \st'=0$. While this is true for some cases such as the points to predicate, this would invalidate useful constructions such as that of an \emph{agreement}, where knowledge can be duplicated. For an agreement separation algebra, we have $\st \cdot \st = \st$, which dissatisfies cancellativity.

This approach is also taken in \cite{sepalgebra}, where separation algebras are defined with a functional ternary relation $J(x,y,z)$ written $x\oplus y=z$ -- the choice of using a relation rather than a partial function being due to the authors wanting to construct their models in Rocq (formerly Coq) \cite{coq}, which only supports computable total functions. While the distinction is mostly syntactical and both relational and functional approaches are equivalent as long at the relation is functional, the former makes proof-work less practical when working with partial functions \cite{statesoundness}, whereas the functional approach allows one to reason equationally \cite{iris}. This paper also introduces the notion of \emph{multi-unit separation algebras}, separation algebras that don't have a single unit $0$, but rather enforce that every state has a unit, that can be different from other states' units. Formally, this means that instead of having $\exists u\ldotp \forall x\ldotp x \oplus u = x$, we have $\forall x\ldotp\exists u_x\ldotp x\oplus u_x=x$ -- this allows one to properly define the disjoint union (or sum) of separation algebras, with both sides of the sum having a distinct unit. We may note that this is a first distinction from PCMs, as a partially commutative monoid cannot have two distinct units; according to the above definition, separation algebras are rather a type of partially commutative semigroup.

In \cite{statesoundness}, further improvements are done to the axiomatisation of separation algebras. Most notably, the property of cancellativity is removed, as it is too strong and unpractical for some cases, as shown previously. Furthermore, the idea of unit, or \emph{core}, is made explicit, with the definition of the total function $\hat-$, the core of a resource, which is its \emph{duplicable~part}.

Finally, Iris \cite{iris1,iris2,iris3,iris} is a state of the art ``higher-order concurrent separation logic''. It places itself as a solution to the problem mentioned in \cite{next700seplogics}, and aims to provide a sound base logic that can be reused and extended to fit all needs. It defines \emph{resource algebras} (RAs), objects that are similar to separation algebras. Because Iris supports ``higher-order ghost state'', more sophisticated mechanisms are needed to model state, and as such the composition function $\cdot$ needs to be total. To rule out invalid compositions, a validity function $\irisval$ is instead used, which returns whether a given state is valid. The core function, written $|-|$, is made \emph{partial}, rather than total -- this, they argue, makes constructing state models from smaller components easier. This was confirmed when developing state models for Gillian, where having the core be a total function made some state models more complicated to make sound, as will be shown later.

\section{Program Verification with Gillian} \label{sec:gillianverif}

While having a logic to prove programs is a great step towards verifying codebases, doing it manually is tedious and time-consuming. Tools have been developed to automate this, such as \emph{Gillian} \cite{gillian0, gillian1, gillian2}. It is a \emph{compositional symbolic execution engine}, with the added property of being \emph{parametric on the memory model}. It allows reasoning about \emph{correctness and incorrectness}. We may now go over exactly what this means.

\subsection{Symbolic Execution}

Traditionally, software is tested by calling the code with a predetermined or randomly generated input (for instance with fuzzing), and verifying that the output is as expected. These approaches, where the code is executed directly, are in the realm of \emph{concrete} execution, as the code is run with concrete (i.e. real, existing) values. While this method is straightforward to execute, it comes with flaws: it is limited by the imagination of the person responsible for writing the tests when written manually, or by the probability of a given input to reach a specific part of the codebase when generated. With fuzzing, methods exist to improve the odds of finding new paths \cite{smartfuzzing}, but it all amounts to luck nevertheless.

A solution to this is symbolic execution: rather than running the code with concrete values, \emph{symbolic} values are used \cite{surveysymex}. These values are abstract, and are  restricted as an interpreter steps through the code and conditionals are encountered. Once a branch of the code terminates, a constraint solver can be used against the accumulated constraints to obtain a possible concrete value, called an \emph{interpretation}. This method appeared in the 70s, notably with the Select \cite{select-system} and EFFIGY \cite{effigy-system} systems.

For a simple C program that checks if a given number is positive or negative and even or odd (see \autoref{fig:branching-example}), symbolic execution would thus branch thrice, once at each condition.\footnote{For simplicity, we omit the case where \code{x} is not an integer, which would lead to an error.} This would then result in 5 different branches, each with different constraints on the program variable \code{x}: $\code{x} = 0$, $\code{x}\%2=0 \land \code{x}<0$, $\code{x}\%2=0 \land \code{x}>0$, $\code{x}\%2\neq0 \land \code{x}<0$ and $\code{x}\%2\neq0 \land \code{x}>0$.\footnote{The four last constraints also contain $\code{x} = 0$, which is included in $\code{x}>0$ and $\code{x}<0$}

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\begin{lstlisting}
if (x == 0) {
	print("zero");
	return;
}
if (x % 2 == 0) {
	print("even");
} else {
	print("odd");
}
if (x < 0) {
	print("negative");
} else {
	print("positive");
}
	\end{lstlisting}
	\caption{Simple branching program}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\begin{forest}
[{$\top$}
	[{$\code{x} = 0$}]
	[{$\code{x}\neq 0$}
		[{$\code{x}\%2=0$}
			[{$\code{x} < 0$}]
			[{$\code{x} \geq 0$}]
		]
		[{$\code{x}\%2\neq0$}
			[{$\code{x} < 0$}]
			[{$\code{x} \geq 0$}]
		]
	]
]
\end{forest}
\caption{Paths obtained from symbolically executing the code, with the added constraint at each node}
\end{subfigure}
\caption{Symbolic execution of a simple program}
	\label{fig:branching-example}
\end{figure}
\subsection{Compositional Symbolic Execution}

An issue that arises from symbolic execution is that is scales quite poorly, because of path explosion \cite{pathexplo,surveysymex}, as each branching can potentially multiply by two the total number of branches. Furthermore, it lacks compositionality, as the code must be tested in its entirety, from \code{main} to \code{return}.

A solution to this is \emph{compositional} symbolic execution (CSE), in which the code is tested by fragments, function by function. This allows for gradual adoption, as only parts of the code base can be tested, all while minimising the effect of path explosion.

A great tool to enable this is separation logic: one can specify the precondition and postcondition of a function, and the symbolic execution can then step through the code, by starting from a state satisfying the precondition and ensuring that once the branch terminates the postcondition is satisfied. Because of the frame rule, function specifications can then be re-used when calling functions, by symbolically executing the function specification on the corresponding state fragment. If the called function does not have a specification that can be used, it can instead be inlined, executing the code within the function, as would standard symbolic execution.

This approach was pioneered in Smallfoot \cite{smallfoot}, and was followed by later tools, such as JaVerT \cite{javert1, javert2}, Infer \cite{infer}, VeriFast \cite{verifast}, SpaceInvaders \cite{spaceinvader} and others -- their individual contributions will be discussed in more detail in the next section.

\subsection{Parametricity}

An innovation of Gillian compared to other existing tools is the fact is is parametric on the memory model (also called \emph{state model}). Typical CSE engines usually come with a built-in notion of memory, ranging from simple heaps with memory chunks \cite{verifast} to more complex state models tailored to a language, like in JaVerT \cite{javert1, javert2} to represent JavaScript objects.

While a fixed state model is useful for specific languages or feature sets, this can become a limitation when porting the engine to other languages. The goal behind Gillian's parametricity is to allow it to be used for virtually any language, as long as the language's state model can be encoded into a separation logic equivalent; improvements done to the engine can then be carried to all of its instantiations for free. This enables it to verify programs written in JavaScript, C or Rust, despite their significant differences in terms of state representation and level of abstraction.

To define a memory model to use in Gillian, a developer must implement it in OCaml~\cite{ocaml}. A memory model is a partially commutative monoid, comprised of a set of states, which can be modified via \emph{actions} and be represented in specifications via \emph{core predicates}.

Actions serve as the interface for programs to interact with the state, for instance by storing or loading values into the state, or allocating a new block of memory and then freeing it. The language Gillian targets, GIL (Gillian's Intermediate Language), provides the base semantics of an imperative goto-language (variables, errors, functions...) which do not affect the state -- actions are the only way of manipulating it.

Core predicate are predicates which have in and out parameters, and represent fragments of the state. They are the encoding of the state into separation logic, and can be used within function specifications. For example, the separation logic $a \mapsto x$ predicate could be represented as a core predicate as \corepred{\pointsto}{\code{a}}{\code{x}}.

\subsection{Correctness, Incorrectness}

Another key difference between Gillian and other CSE engines is that it is capable of reasoning about programs both with correctness, in SL, and incorrectness, in ISL, thus enabling OX program verification and UX true bug-finding respectively. It also supports whole-program symbolic testing, as a non-compositional engine would.

This allows one to instantiate Gillian to their preferred target language, and automatically benefit from all three modes of symbolic execution. This is in part possible thanks to the exact \cite{exactsl} semantics of GIL, enabling automated proofs to be done both with forwards and backwards implications, representing OX and UX respectively.

Thanks to this and the aforementioned parametricity of the memory model, Gillian is therefore a unified CSE engine, that allows scalable verification of software in a unified setting, without requiring one to adapt to two different engines, which may be prohibitively time-consuming.

\section{Existing Tools} \label{sec:existing-tools}

A wide range of engines exist to verify the correctness of code, most targeted towards a specific language. This allows them to implement behaviour that is appropriate for that particular language.
For instance, CBMC \cite{cbmc} is a bounded model checker, that allows for the verification of C programs, via whole program symbolic testing -- that is, it is not compositional, and instead symbolically executes a given codebase, and verifies that assertions hold on a generated formula. While clearly useful, as shown by its success, this solution doesn't scale particularly well; larger applications need to be split modularly to be verified, and manual stubbing is required to support testing parts of the code separately.

Slightly separate from symbolic execution, KLEE \cite{klee} allows for the \emph{concolic} execution of LLVM code -- thus supporting C, C++, Rust, and any other language that can be compiled to LLVM. Concolic execution is a hybrid of symbolic and concrete execution, allowing the use of symbolic values along a concrete execution path. Similarly to CBMC, it does whole program testing, and as such doesn't scale with larger codebases. MACKE \cite{macke} is an adaptation of KLEE enabling compositional testing, proving that concolic execution can be extended to be more scalable, without losing accuracy in reported errors.

As codebases grow, there comes a need for compositional tools that can verify smaller parts of the code, enabling better scaling and integration in CI pipelines, which allow rapidly giving feedback to developers. Separation logic enables this, by allowing one to reason about a specific function and ignoring the rest of the program's state.

While Smallfoot \cite{smallfoot} was the first tool to enable compositional symbolic execution with SL, jStar \cite{jstar} was the first to allow automated verification of a real-world language, Java. It is in particular tailored towards handling design patterns that are commonly found in object-oriented programming and that may be hard to reason about with verification. It uses an intermediate representation of Java called Jimple, taken from the Java optimisation framework Soot. Similarly, VeriFast \cite{verifast} is a CSE tool that is targeted at Java and a subset of C, while SpaceInvader \cite{spaceinvader} allows for the verification of C code in device drivers.

The Verified Software Toolchain \cite{vstiris} also allows verifying correctness of C code using SL; work has been done recently to bring some ideas from Iris such as ghost state into the toolchain. This makes it the only example of engines using Iris.

Infer \cite{infer} is a tool developed by Meta that uses separation logic and bi-abduction to find bugs in C, C++, Java and Objective-C programs, by attempting to prove correctness and extracting bugs from failures in the proof. It is the tool that pioneered \emph{bi-abduction} \cite{biabduction}, a technique enabling execution to proceed despite resources missing, via the construction of an anti-frame. Its approach, while initially stemming from separation logic, was used for finding bugs rather than proving correctness, which led to the creation of ISL \cite{isl} to provide a sound theory justifying this approach. Following this, Pulse \cite{pulse} followed and fully exploited the advances made in ISL to show that this new theory served as more than a justification to past tools, and to prove the existing of true bugs.

Also using separation logic, JaVerT \cite{javert1, javert2} is a CSE engine aimed at verifying JavaScript, with support for whole program testing, verification, and bi-abduction in the same fashion as Infer -- it followed from Cosette \cite{cosette}, which already supported whole program testing.

While the above examples are targeted at specific target languages, some tools have also been designed with modularity in mind. To this end, they instead support a general intermediate language -- one that isn't designed specifically for a language, like Jimple for Java -- that a target language must be compiled to. This allows for greater flexibility in regards to what languages can be analysed by the engine, at the cost of requiring a compiler to the intermediate language.

For instance Viper \cite{viper} is an infrastructure capable of program verification with a permission-based separation logic, akin to what is presented in \cite{fracpermissions}. It's intermediate language is a rich object-oriented imperative typed language, that operates on a built-in heap. A range of Viper frontends exist, allowing for the verification of Scala, Java and OpenCL.

One can still take genericity further, by also abstracting the state model the engine operates on. A tool that does this is coreStar \cite{corestar}, the CSE engine backing jStar with all the Java-related components removed and replaced. It does not target Java, but instead a generic intermediate language. Furthermore, instead of coming with predicates predefined, it requires user-defined predicates to be provided, which are then used throughout the proof. They are defined in a language specified by coreStar, and are thus however quite limited in expressivity. Still, this is to our knowledge the first example of a tool that allows some flexibility in the underlying logic theory.

This is also what Gillian \cite{gillian0, gillian1, gillian2, sacha-phd} does, a CSE engine parametric on the state model. It doesn't support any one language, but instead can target any language that provides a state model implementation in OCaml and a compiler to GIL, its intermediary language. Currently Gillian has been instantiated to the C language (via CompCert-C), JavaScript, and Rust \cite{gillianrust}. It is a generalisation of JaVerT, which only targeted JavaScript. Following Gillian, work has also been done on formalising a theoretical CSE engine, providing axioms an implementation must follow \cite{cse1,cse2} as well as an implementation in Rocq of the engine to prove its soundness.

In \autoref{tab:verif-tools-comparison} we show a side-by-side comparison of the aforementioned tools.

\begin{table}[h]
\caption{Comparison of verification tools}
\begin{tabular}{l|lccl}
&&& \textbf{Parametric} &\\
& \textbf{Target Language} & \textbf{Compositional} & \textbf{State Model} & \textbf{Mode} \\ \hline
CBMC     & C                         & \xmark & \xmark & WPST\\
KLEE     & LLVM                      & \xmark & \xmark & Concolic  \\
MACKE    & LLVM                      & \cmark & \xmark & Concolic \\
jStar*   & Java                      & \cmark & \xmark & OX \\
VeriFast & C, Java                   & \cmark & \xmark & OX \\
coreStar*& IR (coreStarIL)           & \cmark & \cmark & OX \\
Infer    & C, C++, Java, Objective-C & \cmark & \xmark & OX \\
Cosette* & JavaScript                & \cmark & \xmark & WPST \\
JaVerT*  & JavaScript                & \cmark & \xmark & WPST, OX \\
Viper    & IR (Viper)                & \cmark & \xmark & OX \\
Pulse    & C, C++, Java, Objective-C & \cmark & \xmark & UX \\
Gillian  & IR (GIL)                  & \cmark & \cmark & WPST, OX, UX\\
\multicolumn{5}{c}{\footnotesize{WPST = Whole Program Symbolic Testing.}}\\
\multicolumn{5}{c}{\footnotesize{*Unmaintained}}
\end{tabular}
\label{tab:verif-tools-comparison}
\end{table}

\section{Legal, Social, Ethical and Professional Requirements} \label{sec:legal-requirements}

There aren't particular legal, social or ethical considerations associated with compositional symbolic execution. To the contrary, improving code verification and facilitating its use and adoption may result in positive outcomes in a wide range of fields where programs are critical. This includes, among others, medical, nuclear and spatial fields, where program failures can cause life loss, catastrophic environmental degradation, or significant economic loss. For instance, the infamous Therac-25 radiation therapy machine caused the death of four patients and the injury of two others between 1985 and 1987 due to an error when handling shared mutable state \cite{therac25}. This could have been detected and avoided if proper verification of the program had been done. 

Verification tools have the potential to cause harm, if used on open source software by malevolent users that want to find possible exploits.\footnote{Though, to our knowledge, no exploit was found this way.} Exploits such as Heartbleed \cite{heartbleedcve} or more recently the \code{xz} backdoor \cite{xzutilscve} happened on Open Source code, and an attacker could very well automate verification of these libraries privately in an effort to find new exploits. It is thus important to facilitate and democratise verification tools to counteract these actors.





