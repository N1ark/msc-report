\chapter{Theory} \label{chap:theory}

In this chapter we will cover the theoretical foundations for resource algebras in a CSE setting. We will first cover in \cref{sec:theory-state-of-affairs} the current state of the art regarding state models, then in \cref{sec:theory-state-model-ras} what modifications are needed for a CSE to handle RAs. This is followed by a list of simple state models in \cref{sec:theory-state-models}, that can be used to construct more complex state models via transformers, as shown in \cref{sec:theory-state-model-transf}. Finally, in \cref{sec:theory-optim-pmap} we look at the theory justifying some optimisations of the \PMap{} state model transformer.

\section{State of Affairs} \label{sec:theory-state-of-affairs}

\subsection{State Models with PCMs for CSE}

Abstract separation logic historically has mostly used partial commutative monoids (PCMs) \cite{abstractseplogic,sepalgebra,iris1,higherorderseplogic}. These are defined as a tuple ${(M, (\cdot)\colon M\times M\part M, 0)}$, corresponding to the carrier set, composition operator, and a unique identity (or unit) element, which must satisfy the properties highlighted in \autoref{fig:pcm-properties}. PCMs are useful for representing state in SL, as they ``mathematically represent the essential notions of state ownership and ownership transfer'' \cite{abstractpcm}, while being simple and abstract enough that they can represent a variety of concrete states (the traditional case being that of heaps).

\begin{figure}
	\centering
	\begin{align*}
	\tag{PCM-Assoc} \forall a,b,c&\ldotp (a\cdot b)\cdot c = a\cdot (b\cdot c)\\
	\tag{PCM-Comm} \forall a,b&\ldotp a\cdot b = b\cdot a\\
	\tag{PCM-Identity} \forall a&\ldotp a\cdot 0 = a
	\end{align*}
	Equality means either both sides are defined and equal, or both are undefined.
	\caption{Properties of Partial Commutative Monoids.}
	\label{fig:pcm-properties}
\end{figure}

When bringing this concept to a Compositional Symbolic Execution (CSE) engine, the PCM must be endowed with a set of actions $\actions$, of core predicates $\preds$, and functions that allow modifying it; in particular, an \execac{} function that reflects program commands that modify the state, a \produce{} function to add an assertion to the state, a \consume{} function to remove an assertion from the state, and a \fix{} function to provide fixes for missing errors in bi-abduction. In \cite{cse1,cse2,sacha-phd}, the notion of state model is thus introduced, to represent a PCM equipped with these attributes, which can be seen in \autoref{fig:state-model-elements}. A state model is denoted $\mmdl$, and does not include the variable store or the path condition; these are handled separately by the engine.

\begin{figure}\centering
\setlength{\fboxsep}{0.3cm}
\noindent\fbox{\parbox{\textwidth-3cm}{
\textbf{Elements:} \begin{compactitem}
\item $(\St, (\cdot)\colon\St\times \St\part \St, 0)$: PCM of concrete states.
\item $\Sst$: Set of symbolic states.
\item $\actions$: Set of actions.
\item $\preds$: Set of core predicates.
\end{compactitem} \vspace{0.3cm}
\textbf{Functions:} \begin{compactitem}
\item \execac: executes an action with arguments, returns a value.
\item \produce: adds a core predicate assertion into the state, returns the new state.
\item \consume: removes a core predicate assertion from the state, returns the new state and the outs of the assertion.
\item \fix: returns the assertions needed to fix the given missing error.
\end{compactitem}\vspace{0.3cm}
\textbf{Theoretical Properties:}
\begin{compactitem}
\item $\models$: symbolic interpretation relation.
\item $\modelsp$: core predicate satisfiability relation.
\end{compactitem}
}}%
\vspace{0.2cm}
\caption{High-level description of the elements of a state model}
\label{fig:state-model-elements}
\end{figure}

All of these together enable a CSE engine to be \emph{parametric} on the state model; as long as the provided functions satisfy a set of axioms, the engine using the state model can be proven to be sound, either in over-approximate (OX) or under-approximate (UX) mode, in turn allowing for program verification or true bug finding.

\emph{Core predicates} (sometimes simply called predicates) are used to make the assertion language of the engine parametric; they're written ${\delta\in\preds}$. To enable this parametricity, the assertions of the language (see \autoref{fig:assertion-language}) are extended with a core predicate assertion, written $\corepred{\delta}{\ins}{\outs}$, where $\delta$ is the core predicate, $\ins$ are the \emph{in-values} (or ``ins'') of the predicate, and $\outs$ are the \emph{out-values} (or ``outs''). This distinction is used for automation purposes, to create matching plans: they tell the engine that given the values $\ins$, the state can return the corresponding $\outs$ associated with it; this is akin to the ``parameter modes'' described in \cite{parametermodes}. For instance, the traditional ``points to'' assertion $a \mapsto v$, where $a$ is an address and $v$ a value, can be written as $\corepred{\pointsto}{a}{v}$: given the address $a$, a heap can return the value store at the location $v$. Core predicate also imply a satisfiability relation, denoted $\st\modelsp\corepred{\delta}{\ins}{\outs}$ to signify a state satisfies a given predicate -- this is akin to \cite{abstractseplogic,localreasoning}, where predicates over states are a set of states. Predicate satisfiability happens at the concrete level, and is later lifted to the symbolic realm by evaluating the ins and outs using a substitution $\theta$ and variable store $s$. Another traditional example is the \emp{} assertion, that is satisfied by the empty memory; it can be defined as a core predicate: $\corepred{\code{emp}}{}{\!}$.

\begin{remark}[Semantics of emp]
There are two accepted and clashing semantics of the \emp{} assertion; either it is equivalent to $\vtrue$ and any state satisfies \emp{}, or it represents strictly the empty memory and nothing else -- the former is more appropriate for garbage collected languages while the latter for \alloc/\free{} languages \cite{sljungle}.

Here we make the arbitrary choice of defining it as only being satisfied by the empty memory. Because the engine is parametric on the core predicates, a user is free to chose the semantics appropriate to their use case -- this is a strength of the parametric approach.
\end{remark}

Given the predicate satisfiability $\modelsp$, producing a core predicate assertion \corepred{\delta}{\ins}{\outs} onto a state $\st$ is equivalent to composing with it a state $\st_\delta$ such that $\st_\delta\modelsp\corepred{\delta}{\ins}{\outs}$; in other words, we would get \ppprod{\st,\delta,\ins,\outs}{\st\cdot\st_\delta}. Consuming is then extracting the state satisfying the core predicate with the given ins from the state: \ppcons{\st\cdot\st_\delta,\delta,\ins}{\Ok,\st,\outs}.\footnote{\consume{} and \produce{} act on symbolic states; here we treat them as concrete to explain how they work more simply, as this allows us to use composition.}

\begin{figure}
	\begin{align*}
	\sym v &\in \LVal\\
	\sym x &\in \LVar \subset \LVal\\
	\delta &\in \Delta \\
	P, Q, ... \in \Asrt &\defeq \sym v ~|~ \vtrue ~|~ P \Rarr Q ~|~ P \lor Q ~|~ \exists \sym x \ldotp P ~|\\
	&\quad~ P * Q ~|~ \corepred{\delta}{\sins}{\souts}
	\end{align*}
	\caption{Assertion language}
	\label{fig:assertion-language}
\end{figure}

\emph{Actions} are used to represent program actions that interact with the state. Typical actions include \load{}, \store{}, \alloc{} or \free. These actions are called with a list of arguments, return a list of results, and may modify the state, as long as the modification are sound with respect to a set of axioms, presented later. A key property of these state actions is that they are \emph{local}: they only operate on a fragment of state, and don't necessarily need it in its entirety to succeed \cite{abstractseplogic}.

We may now look at an example of function, to understand how the state model is used. Consider the function \code{set\_value} which has the following specification:
\begin{align*}%
\ESLtriple
	{\code{a}=\sym a * \code{x} = \sym x * \corepred{\pointsto}{\sym a}{\sym v}}
	{set\_value(a, x)}
	{\corepred{\pointsto}{\sym a}{\sym x}}
\end{align*}%
If the engine aims to verify the function, it will start with an empty state and \emph{produce} all assertions of the pre-condition (the order being determined by the matching plan). Here, this means adding the assertions $\code{a} = \sym a$ and $\code{x} = \sym x$ to the path condition, and producing the core predicate \corepred{\pointsto}{\sym a}{\sym v} into the state. It will then execute the function's body until it terminates. Finally, to ensure the state at the end of the function matches the postcondition, the assertions are consumed: \corepred{\pointsto}{\sym a}{\sym x} is consumed, effectively removing the binding from the state. 

For the case of function calls using function specifications, the inverse is done: when calling a function its precondition is consumed, and its postcondition is produced, effectively \emph{framing off} the corresponding state, and then framing back on the post condition. For function calls by inlining, the specification is not used; instead, the code of the function is directly executed on the state. See \autoref{fig:function-call-vs-verif} for an illustration.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{diagrams/function-call-vs-verif.drawio.pdf}
	\caption{Function verification and function calling, for a function with precondition~$P$ and postcondition~$Q$.}
	\label{fig:function-call-vs-verif}
\end{figure}

\subsubsection{The Linear Heap State Model}

To exemplify how a state model works, let's consider the traditional linear heap. We use a definition similar to what is shown in \cite{isl}: the two spatial assertions are the cell $a\mapsto x$, and the freed resource $a\freedpt$, meaning the cell at $a$ was freed. We also need four memory-related commands: \load, \store, \alloc{} and \free{}. Our concrete state is a mapping from integers to values or to a special $\varnothing$ value representing a freed state. The symbolic state is then the same, but lifting addresses and values to the symbolic realm. As such, our state model $\mmdl_\statemodel{LinHeap}$ thus defines: \begin{compactitem}
 \item The concrete PCM $\St$, of which the carrier set is $\nats\part \Val \uplus \{\varnothing\}$ -- composition is the disjoint union, and the $0$ is the empty map.
 \item The symbolic states $\Sst = \LVal \part \LVal \uplus\{\varnothing\}$.
 \item The actions $\actions=\{\load,\store,\alloc,\free\}$.
 \item The core predicates $\preds=\{\pointsto, \freedP\}$; we can then write \corepred{\pointsto}{a}{x} and \corepred{\freedP}{a}{} for $a\mapsto x$ and $a\freedpt$ respectively.
 \end{compactitem}

 The definition of \execac{} is straightforward, modifying the state accordingly. \consume{} of \pointsto{} and \freedP{} suceeds and removes the binding appropriately when the mapped-to value is in \LVal{} or is $\varnothing$ respectively. \produce{} does the opposite, adding the binding to the address and vanishing (resulting in no branches) if the binding clashes.

 Finally we define the satisfiability relations: $\models$ is defined with the evaluation of the addresses and values of the symbolic state. We then define $\modelsp$ as ${[a\mapsto x]\modelsp \corepred{\pointsto}{a}{x}}$ and ${[a\mapsto\varnothing]\modelsp\corepred{\freedP}{a}{}}$.

\subsection{Where it goes wrong}

While PCMs are an obvious and straightforward choice for representing state ownership, they come with certain drawbacks. Firstly, PCMs can be overly restrictive; in particular, the presence of a unit $0$ is restrictive, as it makes construction of sums impossible: for that, one needs to allow for multiple units in the object. In \cite{sepalgebra}, it is shown that where a PCM fulfils $\exists u\ldotp \forall x\ldotp x\cdot u = x$, one can allow for more units by simply swapping the existentials: $\forall x\ldotp \exists u_x\ldotp x \cdot u_x = x$. This rule is further relaxed in Iris \cite{iris}, where it is shown that not having a unit at all can also be useful, in particular when one wants to construct \emph{resource algebras} from simpler elements. Finally, and as noted in \cite{iris}, PCMs are ``not enough'' for certain applications, such as when wanting to support higher-order logic.

In \cite{sacha-phd}, the concept of RA constructions from \cite{iris} is brought into a CSE setting, using PCMs. The original work however contained some errors in constructions, that created unsoundness in the verification tool. These errors stemmed, from the most part, to the use of PCMs, and the existence of the unit $0$. Indeed, this made some constructions, such as sum, freeable or the partial map unsound. We will now look at examples of these.

\subsubsection{Unsoundness in Freeable}

The first example is with $\Freeable_\PCM(\mmdl)$. This state model \emph{transformer} is given a state model $\mmdl$, and extends it with the capacity to free state, via the \free{} action. It also adds a \freedP{} predicate, to represent freed memory.

Now consider the simple state model $\Freeable_\PCM(\Ex_\PCM(\Val))$. This represents exactly the pointed-to state in our previous linear heap example. Here $\Ex_\PCM(X)$ is the exclusive state model, that represents exclusive ownership of a value of type $X$.\footnote{Note we write $\Ex_\PCM(X)$ and $\Freeable_\PCM(\mmdl)$ -- the $\Ex_\PCM$ state model accepts a set, $X$, whereas $\Freeable_\PCM$ accepts a state model, $\mmdl$.} Going back to our linear heap, the pointed to cell is also exclusively owned; one cannot have two different values at the same address, we either have it all, or don't have it. Ownership is indicated via the $\exP$ core predicate: \corepred{\exP}{}{x} means the state currently has the value $x$ -- this state model will be further defined later, and is only used here for the example. For now, we define $\Ex_\PCM(X)$~as:
\begin{align*}
	\Ex_\PCM(X) &\defeq \ex{x\colon X} ~|~ 0
\end{align*}

The notation $\ex{x\colon X}$ is equivalent to $\{ \ex{x} : \forall x\in X \}$. We thus define $\Ex_\PCM(X)$ as the set of elements of $X$ \emph{and} the $0$ element. We also define composition such that $0\cdot \st=\st\cdot 0=\st$ and $\ex{x} \cdot \ex{y}$ is always undefined: since the value is owned exclusively, we own it as a whole and nothing can be added to it. This satisfies the axioms of PCMs. We now consider a naive definition of $\Freeable_\PCM(\mmdl)$'s PCM, where $\St$ is the carrier set of $\mmdl$'s~PCM:
\begin{align*}
	\Freeable_\PCM(\mmdl) &\defeq \text{sub}(\st\colon \St) ~|~ \freed ~|~ 0
\end{align*}

We then again define composition as $0 \cdot \st=\st \cdot 0=\st$, and $\text{sub}(\st) \cdot \text{sub}(\st')=\text{sub}(\st\cdot \st')$, which forms a valid PCM.\footnote{All unspecified compositions are considered undefined.} $\Freeable_\PCM(\mmdl)$ exposes a core predicate \freedP, as well as the core predicates of~$\mmdl$; similarly, \consume{} and \produce{} are lifted. The core predicate $\corepred{\freedP}{}{}$ is equivalent to the negative cell assertion $\freedpt$, modulo the index. While we omit the full details of the state model, we note that producing \freedP{} only succeeds when in state $0$ (since a state can't be both freed and something else), producing something other that \freedP{} calls the \produce{} function of the underlying state model $\mmdl$, and stores the resulting state $\st$ as $\text{sub}(\st)$. Similarly, consuming a predicate of $\mmdl$ while in state $\text{sub}(\st)$ calls \consume{} on that predicate with $\st$, and then wraps the result $\st'$ back into~$\text{sub}(\st')$.

Going back to our state model $\Freeable_\PCM(\Ex_\PCM(\Val))$, it thus exposes two predicates, $\exP$ and $\freedP$. Its set of states is $\{\text{sub}(x) : x \in \Val \uplus \{ 0 \} \} \uplus \{ \freed, 0 \}$. To show the unsoundness of our definition, consider calling the \code{dispose} function:
\begin{align*}
	\ESLtriple
		{\corepred{\exP}{}{\sym x}}
		{dispose()}
		{\corepred{\freedP}{}{}}
\end{align*}

Assume we're in the state $\text{sub}(\ex{1})$. To call \code{dispose} by its specification, we first need to consume its precondition: we consume \corepred{\exP}{}{\sym x} from the state. To do this, $\Freeable_\PCM$ calls the \consume{} function of $\Ex_\PCM(\Val)$, with the state $\ex{1}$, which returns $0$. $\Freeable_\PCM$ then wraps this state back, resulting in $\text{sub}(0)$. We then produce the post-condition of \code{dispose}, thus producing \corepred{\freedP}{}{} into $\text{sub}(0)$. This however is not valid, according to the rules outlined earlier! Indeed the state can't both be freed and something else. Of course, $\text{sub}(0)$ represents an empty state, but that means that the \consume{} and \produce{} rules of $\Freeable_\PCM$ must take this into account -- a simple implementation could miss it, resulting in such errors.

This difficulty is in fact caused by the existence of both $0$ and $\text{sub}(0)$ as two distinct states, despite them being the same semantically: empty. To fix this, one must either make $\text{sub}(0)$ invalid, defining it as $\text{sub}(x\colon \St \setminus \{0\})$, or instead remove the definition of $0$ and instead use $\text{sub}(0)$ as the unit of $\Freeable_\PCM$. Either cases require one to be quite careful around the unit, as forgetting about it may lead to unwanted~errors.

\subsubsection{Unsoundness in Sum}\label{sec:unsoundness-in-sum}

The sum state model $\Sum(\mmdl_1, \mmdl_2)$, also written $\mmdl_1 + \mmdl_2$, represents a state where either one of the two sides is owned, but not both. It is useful when one wants to represent states that can transition from one set of states to another -- for instance, $\Freeable(\mmdl)$ can also be defined as $\mmdl + \Ex(\{\freed\})$.

While one of the primary purposes of the sum state model transformer is allowing for transitions between one side of the sum to the other, it is in fact non-trivial to define in a way that is both OX-sound and UX-sound, \emph{particularly when using PCMs}. Consider a transition from a state of $\mmdl_1$ to a state of $\mmdl_2$:
\begin{compactitem}
 \item If one wants the transition to be OX-sound, the right hand side of the sum must forbid the $0$ element of $\mmdl_1$, and for any successfuly swapped state $\st_1$, any attempt to swap from a smaller $\st_1'$ such that $\st_1'\preo\st_1$ must result in a \Miss.
 \item If one wants the transition to be UX-sound, the left hand side of the sum must forbid the $0$ element of $\mmdl_2$, and the resulting state must be \emph{exclusively owned}. Being exclusively owned means the state has no valid frame; this property is taken from \cite{iris}, and can be defined for PCMs as: \begin{align*}
	\isexowned_\PCM~\st \defeq \forall \st'\ldotp \st'\neq 0\implies \neg(\st\disj\st')
\end{align*}
\end{compactitem}

If these properties do not hold, then action is not frame-preserving. The proof that the above properties must hold can be found in \cref{ap:sum-soundness}. Again we note how the presence of a $0$ adds requirements around these constructions, cluttering otherwise elegant definitions.

\subsubsection{Unsoundness in PMap}

Finally, we consider how PCMs can cause unsoundness in the partial map state model transformer, $\PMap(I, \mmdl)$. It maps indices from a domain $I\subseteq\Val$ to a codomain state from~$\mmdl$, by \emph{lifting} all predicates of $\mmdl$ with an additional in-value corresponding to the location $i\in I$ to which the predicate corresponds. All action executions also receive an additional argument with the location of the state the action is executed on.

The linear heap we presented earlier is equivalent to $\PMap_\PCM(\nats, \Freeable_\PCM(\Ex_\PCM(\Val)))$; the core predicate \corepred{\pointsto}{a}{x} (better known as $a\mapsto x$) is then \corepred{\exP}{a}{x}. Similarly, \corepred{\freedP}{a}{} (i.e.~$a\freedpt$) becomes \corepred{\freedP}{a}{}. In both cases, \PMap{} uses the in-value $a$ as the address of the substate. We define the PCM of $\PMap_\PCM$ as: \begin{align*}
	\PMap_\PCM(I, \mmdl) &\defeq (I \finmap \St) \times \pset(I)^?
\end{align*}

We omit the precise details of how exactly it works -- it will be extensively discussed in \cref{sec:theory-state-model-transf}. 

The right hand side of the product, $\pset(I)^?$, represents the domain set of the partial map, in other words the set of addresses that are known to exist -- any address outside of the set \emph{cannot} exist. The~$-^?$ signifies that we extend $\pset(I)$ with the element $\bot$, in which case the domain set is not known. We use it to allow distinguishing \Miss{} errors from out of bounds errors, when accessing a binding the state does not own: if the domain set $d$ is not $\bot$ and the index is not in $d$, then the action must be an out of bounds error. From this follows an invariant: given a state $(h,d)$ where $h$ is the heap and $d$ the domain set, $d\neq\bot\Rarr\dom(h)\subseteq d$ must always hold; in other words, we cannot own any state outside of the domain set. The domain set is \emph{exclusively owned}: if two \PMap{} states are disjoint, they can't both have a defined domain set; this allows it to be \emph{modified}, for instance during allocation. The core predicate $\corepred{\domainset}{}{d}$ is satisfied by the state with exactly the domain set $d$. When producing the $\domainset$ core predicate onto a state, the \produce{} function of $\PMap_\PCM$ must thus ensure the invariant constraint is upheld, vanishing otherwise.

$\PMap_\PCM(I,\mmdl)$ lifts all core predicates and actions with an index, and as such all \consume{} and \produce{} calls must also have the index in their ins. The behaviour of a naive implementation would then be similar to what was explained for $\Freeable_\PCM$: calling \consume{} or \produce{} when the binding is not present executes it against the $0$ element of the underlying PCM, and otherwise calls it on the state at the location and stores the result back at that same location, overriding the previous binding.

For this example we consider the SL linear heap state model, $\PMap_\PCM(\nats, \Ex_\PCM(\Val))$.\footnote{We use the SL linear heap, that does not have the freed assertion $a\freedpt$, for simplicity.} For simplicity, we re-use the traditional notation for cells, writing $a \mapsto x$ for \corepred{\exP}{a}{x}.

Now, consider the following function specification that moves a value:
\begin{align*}
\ESLtriple
	{1\mapsto 1 * \corepred{\domainset}{}{\{1\}}}
	{move()}
	{2\mapsto 1 * \corepred{\domainset}{}{\{2\}}}
\end{align*}
This specification is valid, as it is frame preserving. More precisely, because the domain set of the precondition is $\{1\}$ and we own the cell at address $1$, the only state compatible with the precondition is the empty state $([], \bot)$; the same also applies for the postcondition. As such, one should be able to soundly call this function.

Let the initial state be $([1\mapsto \ex{1}], \{ 1\} )$: a heap with one cell, at address $1$ -- this matches the precondition of \code{move}. If we want to call \code{move} by specification, the engine must first consume the entire precondition, assertion by assertion (effectively framing it off). It first consumes $1\mapsto 1$, or \corepred{\exP}{1}{1}; \PMap{} in turn gets the state at address $1$, $\ex{1}$, and consumes \corepred{\exP}{}{1} from it, unlifting the index from the assertion. The pointed-to state then becomes the unit $0$ of $\Ex_\PCM(\Val)$, and the outer state becomes~$(\{ 1\mapsto 0\}, \{ 1\})$. The \domainset{} predicate is then successfully consumed too, leaving ${(\{ 1\mapsto 0\},\bot)}$, since the domain set must be consumed as a whole. 

Next, the post-condition is produced onto the state. First, the engine produces $2\mapsto 1$, or \corepred{\exP}{2}{1}, which adds a binding to $\PMap_\PCM$ and creates the cell: our bigger state is now $([1\mapsto 0, 2\mapsto \ex1],\bot)$. Finally, \corepred{\domainset}{}{\{2\}} is produced onto the state, however this \emph{doesn't succeed}. Indeed, we have $\dom([1\mapsto 0, 2\mapsto \ex1])=\{1,2\}$, and of course $\{1,2\}\not\subseteq \{2\}$: the domain of the heap is not a subset of the produced domain set $\{2\}$. This is because again we forgot to check if the state at location $1$ became $0$ when consuming \corepred{\exP}{1}{1}. We would thus need to check for this, and potentially redefine the PCM of $\PMap_\PCM$ to forbid mappings to $0$. Another solution is to reformulate the invariant, such that indices pointing to $0$ are ignored, since they're empty anyways. In both cases, we must be careful about how we choose to handle the unit.

These examples hopefully showed that PCMs enforce a unit that is unwieldy, and can easily cause unsound behaviour when mishandled. Even when taken into account, it leads to unpleasant definitions, as every state model must define a $0$, and every state model transformer must then either exclude it in its construction, or explicitly take it into account. What if we could simply eliminate the $0$ entirely, avoiding the need to handle it?

\subsection{RAs for Iris}

Iris \cite{iris1,iris2,iris3,iris} departs from the tradition of PCMs, and introduces resource algebras (RAs) to model state, defined as a tuple ${(M, \irisval\colon M\rarr \text{iProp}, |-|\colon M\rarr M^?, (\cdot)\colon M\times M\rarr M)}$, containing the carrier set, a validity function, a partial core function and a \emph{total} composition operator. This definition makes use of the option RA $M^?\defeq M\uplus\{\bot\}$ \cite{iris-option}, that extends the set of $M$ with an ``empty'' element $\bot$, such that $m\cdot\bot=\bot\cdot m=m$. Similarly to PCMs, a set of axioms must be satisfied, as seen in \autoref{fig:irisra-properties}.

The \emph{core function} $|-|\colon M\rarr M^?$ associates to an element $m$ of the RA it's \emph{duplicable core}. The crucial difference here, compared to PCMs, is that aside from allowing multiple units, it allows having \emph{no unit}, in which case $|m|=\bot$. As we will see later, having no unit is a requirement for a state to be exclusively owned (i.e. it has no frame). Indeed, if the state $m$ has a core $|m|\neq\bot$, then that core always constitutes a valid frame.

The \emph{validity function} $\irisval\colon M\rarr \text{iProp}$ is used to rule out invalid compositions, rather than relying on the partiality of the composition operator. It returns an Iris property, \text{iProp}, which allows for step indexing and more sophisticated logic, that is out of the scope of this project. Most RAs are then extended with an invalid element $\lightning$, and validity is often defined as $\irisval(m)\defeq m\neq \lightning$. For instance for the $\Ex_\Iris$ RA: \begin{align*}
	\Ex_\Iris(X) &\defeq \ex{x\colon X} ~|~ \lightning \\
	\irisval(a) &\defeq a \neq \lightning \\
	|\ex{x}| &\defeq \bot\\
	a \cdot b &\defeq \lightning
\end{align*}

\begin{figure}
An \emph{Iris resource algebra} is a tuple $(M, \irisval\colon M\rarr \text{iProp}, |-|\colon M\rarr M^?, (\cdot)\colon M\times M\rarr M)$
\begin{align}
	\tag{Iris-RA-Assoc}
	\forall a,b,c&\ldotp (a\cdot b)\cdot c = a\cdot (b\cdot c)\\
	\tag{Iris-RA-Comm}
	\forall a,b&\ldotp a\cdot b = b\cdot a\\
	\tag{Iris-RA-Core-ID}
	\forall a&\ldotp |a| \in M \Rarr |a|\cdot a = a\\
	\tag{Iris-RA-Core-Idem}
	\forall a&\ldotp |a|\in M\Rarr ||a|| = |a|\\
	\tag{Iris-RA-Core-Mono}
	\forall a,b&\ldotp |a|\in M\land a\preo b \Rarr |b| \in M \land |a| \preo |b|\\
	\tag{Iris-RA-Valid-Op}\label{eq:iris-ra-valid-op}
	\forall a,b&\ldotp \irisval(a\cdot b)\Rarr \irisval(a)
\end{align}
\begin{align*}
	\text{where}\qquad
	M^? &\defeq M \uplus \{\bot\}\text{, with }a\cdot \bot \defeq \bot\cdot a\defeq a\\
	a \preo b &\defeq \exists c\ldotp b = a\cdot c\\
	a \rightsquigarrow B &\defeq \forall c^? \in M^?\ldotp \irisval(a\cdot c^?) \Rarr \exists b\in B\ldotp \irisval(b\cdot c^?)\\
	a \rightsquigarrow b &\defeq a \rightsquigarrow \{b\}
\end{align*}
A \emph{unital} resource algebra is an RA $M$ with an element $\epsilon\in M$ such that:
\begin{align*}
	\irisval(\epsilon) &&
	\forall a\in M. \epsilon\cdot a = a&&
	|\epsilon|=\epsilon
\end{align*}
\caption{Definition of resource algebras in Iris}
\label{fig:irisra-properties}
\end{figure}

The validity and core functions thus make Iris states more powerful, as they have more flexibility in what can be expressed; while a regular PCM can be trivially converted to an RA, the opposite is not true. An example of construction that can easily be more easily constructed is the sum state model. The requirement for a transition from one side of the sum to the other can be more elegantly expressed, without having to worry about the existence of the unit $0$ \cite{iris}. Given ${\text{exclusive}_\Iris(a)\defeq \forall b\ldotp \neg\irisval(a\cdot b)}$, we have:\begin{mathpar}
\inferrule[ExclusiveUpdate]{\text{exclusive}_\Iris(a) \\ \irisval(b)}{a \rightsquigarrow b}
\end{mathpar}

The Iris update $a \rightsquigarrow b$ is a form of frame preserving update: if we update $a$ into $b$, we know that any frame that was compatible with $a$ remains compatible with $b$. For the above rule, because $a$ is owned exclusively, we can replace it with any state, as $a\rightsquigarrow b$ is vacuously true.

Furthermore, Iris is a battle-tested logic,\footnote{See \url{https://iris-project.org/\#publications}, where more than a hundred publications are cited as using Iris, allowing for the verification of, among others, C, Go, OCaml, Rust, Scala or WASM.} that comes with plenty of proofs and properties making them easy to use and adapt, whereas PCMs can prove unwieldy even for simpler state models, as shown previously.

We note an interesting similarity between Iris and the PCM approach however, which is that ``the global RA must be unital, which means it should have a unit element $\epsilon$'' \cite{iris}. This is similar to what one would find in a PCM, where we have the $0$ element. Any RA can be trivially extended to have a unit, via the option RA. This in particular means that while the building blocks of both approaches are different (PCMs or RAs), the end result that is used is similar. This is good news: it means that adapting an RA construction to a CSE engine that uses PCMs should be fairly straightforward, as one can rely on the added unit as a de facto $0$.

\section{State Models with RAs} \label{sec:theory-state-model-ras}

We have seen how PCMs are unpractical for a CSE engine, as they easily create incorrect behaviour that can be quite tricky to notice. We will now see how we may adapt a CSE to instead use RAs. We first define the notion of partial RAs, followed by the description of the changes done to the engine's layers to support partial RAs. Finally, we list the new axioms that must be respected by the functions of the state model.

\subsection{Partial RAs}

A property of Iris RAs is that composition is \emph{total} -- to take into account invalid composition, states are usually extended with a $\lightning$ state, such that $\irisval(\st)\Leftrightarrow \st\neq\lightning$. While this is needed in the Iris framework for higher-order ghost state and step-indexing, this doesn't come into play when one is only manipulating RAs. As such, because having to define and later verify the validity of states can be quite unwieldy, we remove validity by instead making use of partiality, as was originally the case in PCMs. This translation is done by removing the invalid state $\lightning$ when present, and instead defining compositions that lead to it as undefined. From this, we can get rid of the validity function, and define composition as being partial: $(\cdot)\colon M\times M\part M$. This is also in line with the core function $(-)$ being partial.

\begin{figure}
A \emph{partial resource algebra} (RA) is a triple $(M, |\!-\!|\colon M\rarr M^?, (\cdot)\colon M\times M \part M)$
\begin{align}
	\tag{RA-Assoc} \forall a,b,c&\ldotp (a\cdot b)\cdot c = a\cdot (b\cdot c)\\
	\tag{RA-Comm} \forall a,b&\ldotp a\cdot b = b\cdot a\\
	\tag{RA-Core-ID} \forall a&\ldotp |a| \in M \Rarr |a|\cdot a = a\\
	\tag{RA-Core-Idem} \forall a&\ldotp |a|\in M\Rarr ||a|| = |a|\\
	\tag{RA-Core-Mono} \forall a,b&\ldotp |a|\in M\land a\preo b \Rarr |b| \in M \land |a| \preo |b|
\end{align}
\begin{align*}
	\text{where}\qquad
	M^? &\defeq M \uplus \{\bot\}\text{, with }a\cdot \bot \defeq \bot\cdot a\defeq a\\
	a \preo b &\defeq \exists c\ldotp b = a\cdot c\\
	a\disj b &\defeq a \cdot b \text{ is defined}
\end{align*}
A \emph{unital} resource algebra is a resource algebra $M$ with an element $\epsilon\in M$ such that:
\begin{align*}
	\forall a\in M\ldotp \epsilon\cdot a = a&&
	|\epsilon|=\epsilon
\end{align*}
\caption{Definition of (partial) resource algebras}
\label{fig:ra-properties}
\end{figure}

\begin{remark}[Partiality and the option RA]
	One may note that we use the term ``partial'' to refer to two different signatures; the core $|-|\colon M \rarr M^?$ is called partial but is in fact total, with undefined mappings being equal to $\bot$, while composition $(\cdot)\colon M\times M\part M$ is a ``proper'' partial function.

	While one could adapt the core to be $M\part M$, this would prove unpractical, in cases where one still wants to use $|m|$ even when it is $\bot$. On the other hand, defining composition as $M\times M\rarr M^?$ is simply incorrect: let $a\cdot b$ undefined, it is untrue that $(a\cdot b)\cdot c=c$, while defining an undefined composition to equal $\bot$ would render this statement true.
\end{remark}

\emph{Partial} RAs are equivalent to regular RAs, as long as ${\neg\irisval(a)\Rarr \neg\irisval(a\cdot b)}$ holds; luckily for us, this is equivalent to the axiom \eqref{eq:iris-ra-valid-op}. Compositions that yield $\lightning$ (or any state $m$ such that $\neg\irisval(m)$) can be made undefined, and the validity function removed, to gain partiality, and inversely to go back to the Iris definition.

An interesting property of this change is that because validity is replaced by the fact composition is defined, the validity of a composition $\irisval(a\cdot b)$ in a RA is exactly equivalent to the fact two elements are disjoint in a partial RA $a \disj b$. We now define the properties of partial RAs taking this change into account -- see \autoref{fig:ra-properties}. From now, the term RA will be used to refer to these partial RAs.

We may compare the definition of $\Ex_\Iris$, presented earlier, with the equivalent definition using partial RAs, $\Ex$. The main difference is the removal of $\irisval$ and the $\lightning$ element, making the definition simpler (see \autoref{fig:comp-def-ex}).

\begin{figure}
\noindent\begin{minipage}{.5\linewidth}
\begin{align*}
	\Ex_\Iris(X) &\defeq \ex{x\colon X} ~|~ \lightning \\
	\irisval(a) &\defeq a \neq \lightning \\
	|\ex{x}| &\defeq \bot\\
	a \cdot b &\defeq \lightning
\end{align*}\end{minipage}%
\begin{minipage}{.5\linewidth}
\begin{align*}
	\Ex(X) &\defeq \ex{x\colon X} \\
	|\ex{x}| &\defeq \bot\\
	a \cdot b & \text{ is always undefined}
\end{align*}\end{minipage}
\caption{Comparison of the definition of \Ex{} with Iris and partial RAs}
\label{fig:comp-def-ex}
\end{figure}


\subsection{Core Engine}

We're now interested into what changes need to be brought to our CSE to handle RAs. To do this, we use the axioms and definitions stated in \cite{cse2}. We will also specify additional changes that exist in Gillian and that were formalised in \cite{sacha-phd}, but that haven't been added to the aforementioned formalisation.

This CSE engine is split into three layers, each adding functionality to the layer below, allowing for a clear separation of concerns. These layers are, from bottom to top: the core engine, the compositional engine, and the bi-abduction engine (see \autoref{fig:layers-cse-engine}).

\begin{figure}
	\centering
	\includegraphics[width=14cm]{diagrams/engine-layers.drawio.pdf}
	\caption{Layers of the CSE engine}
	\label{fig:layers-cse-engine}
\end{figure}

The core engine enables whole-program symbolic execution. For this, state models must firstly define the set of states the execution will happen on. While \emph{concrete} compositional state is modelled as an RA, \emph{symbolic} compositional state is simply a set of elements that does not come with composition or cores. This is because when one handles symbolic values, composition isn't a partial function any more, as it can branch, and thus output more than one value. Take for instance the RA of trees, where nodes are defined as having an offset and size. Composing two trees made of a single node with a symbolic offset may lead to different results, depending on the interpretation of the offsets: are the two nodes adjacent? Which is on the left or the right? This cannot be modelled with an RA. Soundness of the engine is instead verified by unlifting symbolic states to concrete states via symbolic interpretation~($\models$), and then using the properties of these concretes states, which do form a valid RA.

Once the set of symbolic states $\Sst\ni\sst$ is defined, it must be further equipped with a set of actions~$\actions$ and an $\execac$ function that allows the program to modify the state. We also include the definition of the $\models$ relation, called \code{sat}, noting that this is a purely theoretical construct that isn't implemented in the actual engine.
\begin{align*}
	\execac &\colon \St^? \rarr \actions \rarr \vallist \rarr \pset(\outcomes_e \times \St^? \times \vallist)\\
	\execac &\colon \Sst^? \rarr \actions \rarr \lvallist \rarr \pset(\outcomes_e \times \Sst^? \times \lvallist \times \Pc) \\
	\code{sat} &\colon \Theta \rarr \text{Store} \rarr \Sst \rarr \pset(\St)
\end{align*}

We define two different versions of \execac{}: one operating on concrete states, and one operating on symbolic states. The concrete version of the function is used in the theory, to prove the soundness of its symbolic counterpart, while the symbolic version is what is actually used by the engine to execute the analysed program.

In the concrete case, the arguments of \execac{} are, in order: the \emph{optional} state the action is executed on, the action, and the received arguments. It returns a set of branches, with the outcome, the new state and the returned values. Note how concrete semantics are non-deterministic; this is needed for instance for allocation, which will be discussed in \cref{sec:theory-state-model-transf}. It is pretty-printed as \ppexecc{\alpha}{\st,\ins}{o, \st', \outs}.

For its symbolic counterpart, the arguments are lifted to the symbolic realm. It also returns a set of branches, along with a path condition (PC) $\pc$ representing the condition for the branch to exist -- if the PC is not satisfiable, the branch must be cut. It is pretty-printed as \ppexec{\alpha}{\sst,\sins}{o, \sst', \souts, \pc}.

Here, the outcome is in the set of \emph{execution outcomes} $\outcomes_e=\{ \Ok,\Err,\Miss \}$. Where $\Ok$ represents successful execution and $\Err$ program errors, $\Miss$ outcomes come from the compositional nature of the state. An action that leads to a $\Miss$ doesn't represent a bug in the code, but rather that additional state is needed to determine whether execution is successful or not.

The main difference with \cite{cse2} is that the state may be $\bot$, if the action is executed on empty state; this is made explicit from using $\Sst^?$ (rather than $\Sst$). This ensures non-unital RAs are not ruled out as invalid -- indeed, many useful RAs are not unital and sometimes don't have a unit at all, as is the case for instance for \Ex.

We also modify the definition of a path condition. It is typically defined as a logical value $\LVal$ that must evaluate to a boolean, and which is extended via conjunction. Internally, the engine however still needs to split the conjunction into a list of terms to manipulate it. As such, to make the theory closer to what is effectively done within the engine, we make the choice of defining the path condition as a \emph{list of logical values}: $\pc\in\Pc=\lvallist$. Extension is then defined as concatenating new logical values to the path condition, and strengthening of the path condition can be trivially checked with $\pc'\supseteq\pc$. We further define the predicate \code{SAT}, that is true if, given the substitution $\theta$, store $s$ and a path condition $\pc$, the conjunction of the elements of $\pc$ resolves to $\vtrue$ once evaluated:
\begin{align*}
	\SAT(\pc) \defeq \left\llbracket \bigwedge_{i}^{|\pc|}\pc(i) \right\rrbracket_{\theta,s} \hspace{-1em} = \vtrue
\end{align*}

\begin{remark}[Formality of fresh variables]
	The original definition of the symbolic \execac{} presented in \cite{cse2} also has an $\SV\in\pset(\LVar)$ argument, which is the set containing all existing symbolic variables. This set is used when an action needs to create a fresh symbolic variable $\sym v$, to ensure the variable is fresh by checking $\sym v\notin \SV$. While necessary for proofs within the engine in Rocq, we omit it here, as it is only relevant for one action (\alloc), and pen and paper proofs do not require the same rigidity as a Rocq proof. Including it would be straightforward.
\end{remark}

Finally, the user must define the \code{sat} relation, relating concrete and symbolic states. It is pretty printed as $\theta,s,\st\models \sst$ for $\st\in\code{sat}~\theta~s~\sst$, meaning that given a substitution $\theta$ and a store $s$, the concrete state $\st$ can be matched by a symbolic state $\sst$. The user must only define this relation for non-$\bot$ states; we can then lift it to the option RA, by simply adding that $\forall \theta, s\ldotp \code{sat}~\theta~s~\bot=\{\bot\}$. This relieves users from needing to take $\bot$ into account when defining $\models$ and from proving the axiom \ref{eq:empty-mem}.

\subsection{Compositional Engine}

The compositional engine, built on top of the core engine, allows for verification of function specifications, and handles calls by specification. The state model must be extended with a set of core predicates $\preds$ and a pair of \consume{} and \produce{} functions (equivalent, respectively, to a resource assert and assume). Finally, to link core predicates to states, a core predicate satisfiability relation $\modelsp$ (called $\code{sat}_\preds$) must be defined in the theory, to prove soundness of \consume{} and \produce.
\begin{align*}
	\text{for }M=\{\OX, \UX\}\\
	\consume &\colon M\rarr \Sst^? \rarr \Delta \rarr \lvallist \rarr \pset(\outcomes_l \times \Sst^? \times \lvallist \times \Pc) \\
	\produce &\colon \Sst^? \rarr \Delta \rarr \lvallist \rarr \lvallist \rarr \pset(\Sst^? \times \Pc)\\
	\code{sat}_\Delta &\colon\St^? \rarr \Delta \rarr \vallist \rarr \pset(\vallist)
\end{align*}
Similarly to \execac, the input state can be $\bot$. While intuitively one may assume that the input state of \consume{} and the output state of \produce{} may never be $\bot$ (as there must be a non-empty state to consume an assertion from or produce an assertion into), this would limit what core predicates can do. In particular, this means an $\emp$ predicate couldn't be defined, since it's production on an empty state results in an empty state.

The arguments of \consume{} are, in order: the mode of execution, to distinguish between under-approximate and over-approximate reasoning, the state, the core predicate being consumed, the ins of the predicate. It outputs a \emph{logical outcome}, the state with the matching predicate removed (which may result in an empty state $\bot$), the outs of the predicate and the associated path condition. It is pretty-printed as \ppcons{m,\sst,\delta,\sins}{o, \sst_f, \souts, \pc}, and when the consumption is valid for both \OX{} and \UX{} the mode is omitted.

For \produce{}, the arguments are the state, the core predicate being produced, the ins and the outs of the predicate, resulting in a set of new states and their associated path condition. As an example, producing $\sym x \mapsto 0$ in a state $[1 \mapsto 2]$ results in a new state $[1\mapsto 2, \sym x \mapsto 0]$ with the path condition $[\sym x \neq 1]$. If the produced predicate is incompatible with the state (e.g. producing $1 \mapsto y$ in a state containing $1 \mapsto x$), the producer \emph{vanishes}. Inversely, if the assertion can be interpreted in several ways, the producer may branch. It is pretty-printed as $\ppprod{\sst,\delta,\sins,\souts}{\sst', \pc}$.

The $\code{sat}_\preds$ relation relates a possibly empty \emph{concrete} state, core predicate and in-values to a set of out-values. It is pretty-printed as $\st \modelsp \corepred{\delta}{\ins}{\outs}$ for $\outs \in \code{sat}_\preds~\st~\delta~\ins$. For instance in the linear heap state model, we have $[ 1 \mapsto 2 ] \modelsp \corepred{\pointsto}{1}{2}$. Denoting expression evaluation as $\expeval{\sym e}=e$, we lift $\code{sat}_\preds$ to the symbolic realm:
\begin{align*}
	\theta,s,\st\modelsp \corepred{\delta}{\sins}{\souts} \defeq \expeval{\sins}=\ins \land \expeval{\souts}=\outs \land \st\modelsp \corepred{\delta}{\ins}{\outs}
\end{align*}

Here we define logical outcomes $\outcomes_l = \{\Ok, \LFail, \Miss \}$. These are outcomes that happen during reasoning; in particular, \LFail{} equates to a logical failure due to an \emph{incompatibility} between the consumed predicate and the state. For instance, consuming $1 \mapsto 1$ from a state $[1 \mapsto 2]$ would yield a \LFail{}, while consuming it from state $[2 \mapsto 1]$ would yield a \Miss{}, as the state $[1\mapsto 1]$ could be composed with it to yield a non-miss outcome.

An addition to what \cite{cse2} previously defined, taking inspiration from \cite{sacha-phd}, is thus the split of what was the \code{Abort} outcome into \LFail{} and \Miss{}, which improves the quality of error messages and allows \emph{fixing} consumptions that yield a \Miss{} -- this will be described in the next subsection. This means function calls by specification with missing resources can be done in bi-abduction.

\begin{remark}[\Miss{} in \consume]
	It is argued in \cite{cse1} that \consume{} shouldn't report \Miss, as the missing resource might not actually be used by the function, which would then be UX unsound; we believe this preoccupation is not linked to \consume{} but to function call semantics. If including \Miss{} here is problematic, it should be up to the engine to correctly abort when doing function calls by specification in UX mode, rather than to \consume{} to unnecessarily limit its capabilities.
\end{remark}

\begin{figure}
	\centering
	\includegraphics[width=5cm]{diagrams/outcomes-overlap.drawio.pdf}
	\caption{Overlap of outcomes}
	\label{fig:outcomes-overlap}
\end{figure}

We thus have $\outcomes_e$ for execution outcomes, and $\outcomes_l$ for reasoning outcomes -- one is returned by action execution, and one for predicate consumption (see \autoref{fig:outcomes-overlap}). They overlap on one particular feature of the engine, which is function calls by specification: when calling a function, any outcome can happen: $\Ok$ if the function call succeeds, $\Err$ if the function call faults (as defined by its specification), $\Miss$ if additional resources are needed to satisfy the precondition of the function specification, and $\LFail$ if the state is incompatible with the function specification.

A last change done to what \cite{cse2} defines is that the path condition input of  \consume{} and \produce{} is removed, disallowing both functions from inspecting the path condition -- the functions instead directly return the path condition required for the resulting branches. This brings several advantages: firstly, it simplifies the axioms for \consume{} and \produce{}, as there is no need to require that they strengthen the PC. Instead, the returned PC is always concatenated with the current PC, ensuring it can only be strengthened. Secondly, it simplifies the definition of \consume{}, as whether the branch is cut or not (to satisfy \OX{} and \UX{} soundness) is also delegated to the engine. For instance, in UX all consumption branches that result in \LFail{} can be dropped, as dropping branches is UX-sound -- in OX however, this is not sound. A typical pattern in \consume{} implementations is thus to evaluate the modified PC, and if an \LFail{} is reached drop the branch depending on the mode. By omitting the PC from the arguments, this responsibility is thus delegated to the engine. Finally, this makes a particular optimisation of the engine simpler to implement: incremental SMT solving.\footnote{Z3, which is used in Gillian, uses ``scopes'' for this -- see \url{https://microsoft.github.io/z3guide/docs/logic/basiccommands/\#using-scopes}} This is a mechanism that allows storing an intermediary state in the SMT solver (a ``checkpoint''), adding to the state, and later backtracking to the checkpointed state to evaluate another branch if needed. By ensuring \consume{} and \produce{} only return what to append to the PC, the checkpoint can easily be added before the call, and then each branch's PC added and evaluated. Allowing \consume{} and \produce{} to modify the PC directly would mean the PC must be parsed and filtered, to tell apart old from new terms -- modified terms would make this even harder.

\subsection{Bi-Abduction Engine}

To support bi-abduction in the style of Infer:Pulse \cite{pulse}, \Miss{} outcomes must be fixed. This enables true UX bug finding in under-specified or unspecified functions. For this, the state model must provide a \fix{} function, that given the details of a miss error (these details being of type \lvallist{} and returned with the outcome) returns a \emph{list of sets of assertions} that must be produced to fix the missing error.
\begin{align*}
	\fix \colon \lvallist \rarr \pset(\Asrt) ~\code{list}
\end{align*}
A \emph{list} of different fixes is returned, which themselves are a set of assertions -- this is because, for a given missing error, multiple fixes may be possible which causes branching. For instance, trying to load a cell that is not found in the state can lead to two possible fixes: either the cell is present, and the fix is $\{ \corepred{\pointsto}{\sym a}{\sym x} \}$, either the cell is not present any more because it has been freed (which would then lead to a use after free error), giving the fix $\{{\corepred{\code{freed}}{\sym a}{}}\}$.

Furthermore, we define fixes as assertions rather than lists of core predicates. This is because a fix often contains \emph{more than just spatial information}. The simplest example is the fix for a missing cell at address $\sym a$ in the linear heap: we must ensure the fix can be soundly applied to an existing state, without any variable clashes. For this, we must use an existential, giving us the fix $\exists \sym x\ldotp \corepred{\pointsto}{\sym a}{\sym x}$. More generally, it is sometimes needed that the fix contains logical information, for instance specifying constraints on a variable.

\subsection{Axioms}

We may now go over the axioms that must be respected by the above defined functions for the soundness of the engine. Note we only mention the axioms related specifically to the state models -- axioms relating to the overall soundness of the engine or the core semantics are out of the scope of this report.

For all of the axioms we assume we have a symbolic state model $\mmdl$, made of the concrete states RA $\St \ni \st$, symbolic states set $\Sst \ni \sst$, actions $\actions$ and core predicates $\preds$.

We split the axioms into two parts: those relating to the symbolic nature of state and ensuring soundness with respect to a concrete state, and those relating to the parametricity of the state.

\subsubsection{Symbolicness Axioms}

\begin{equation}
\tag{Empty Memory}\label{eq:empty-mem}
\theta,s,\st\models \bot \iff \st = \bot
\end{equation}

The above axiom is obtained for free, by lifting the $\models$ relation to the option RA.

\begin{equation}
\tag{Memory Model OX Soundness}\label{eq:mem-ox-soundness}
\begin{array}{l}
\ppexecc{\alpha}{\st,\ins}{o,\st',\outs} \land \theta,s,\st\models\sst \land \expeval{\sins}=\ins \implies \exists \sst',\souts,\pc,\theta'\ldotp\\
\quad \ppexec{\alpha}{\sst,\sins}{o,\sst',\souts,\pc} \land \SAT[\theta',s](\pc) \land \theta',s,\st'\models \sst' \land  \expeval[\theta',s]{\souts}=\outs
\end{array}
\end{equation}

\begin{equation}
\tag{Memory Model UX Soundness}\label{eq:mem-ux-soundness}
\begin{array}{l}
\ppexec{\alpha}{\sst,\sins}{o,\sst',\souts,\pc} \land \SAT[\theta',s](\pc) \land \theta,s,\st'\models\sst' \land \\
\expeval[\sym s,\pc]{\souts}\rightsquigarrow(\outs,\pc')\land \expeval[\sym s,\pc']{\sins}\rightsquigarrow(\ins,\pc'')\implies\\
\quad \exists \st\ldotp \ppexecc{\alpha}{\st,\ins}{o,\st',\outs} \land \theta,s,\st\models\sst
\end{array}
\end{equation}

The above two axioms are at the core of OX and UX soundness, respectively. The former ensures that if a concrete execution exists, then it must also exist in the symbolic semantics; it however doesn't ensure that more executions that those that actually exist may happen. The latter ensures that if a symbolic execution exists, then it must also exist in the concrete world; this time, no guarantees are made regarding the existence of such symbolic transition: for instance, symbolic semantics that never result in any branches are UX-sound (though they are of limited use).

Because incorrectness separation logic does execution backwards, we also prove UX soundness of the memory model backwards: if the resulting state has a concrete model, then so must the state before. In OX, soundness is instead forwards: if the starting state is sound, so must be the resulting state.

\subsubsection{Compositionality Axioms}

\begin{equation}
\tag{Frame subtraction}\label{eq:frame-sub}
\begin{array}{l}
\st\disj\st_f \land \ppexecc{\alpha}{\st\cdot\st_f,\sins}{o, \st',\outs} \implies \\
\quad\exists \st'', o', \outs'\ldotp \ppexecc{\alpha}{\st,\ins}{o', \st'', \outs'} ~\land \\
\qquad(o' \neq \Miss \implies o' = o \land  \outs' = \outs \land \st' = \st'' \cdot \st_f)
\end{array}
\end{equation}

\begin{equation}
\tag{Frame Addition}\label{eq:frame-add}
\begin{array}{l}
\ppexecc{\alpha}{\st,\ins}{o, \st', \outs} \land o \neq \Miss \land \st'\disj\st_f\implies\\
\quad\st \disj \st_f \land \ppexecc{\alpha}{\st\cdot\st_f, \ins}{o, \st'\cdot\st_f, \outs}
\end{array}
\end{equation}

The above two axioms are what enable the traditional frame rule of separation logic, but bringing it to operational semantics. Frame subtraction in particular is almost identical to the ``Frame property'' defined in \cite{localreasoning}: if executing an action on a state from which a frame was removed yields a non-\Miss, then that action did not need that frame, and the frame can be added to the result. For frame addiction, which is needed in UX, we go backwards again: if a frame can be added to the state after executing an action and the action doesn't cause a \Miss, then it can also be added to the state before without interfering with the action execution.

\begin{equation}
\tag{Consume OX Soundness}\label{eq:consume-ox-sound}
\begin{array}{l}
\ppcons{\sst,\delta,\sins}{\Ok,\sst_f,\souts,\pc} \implies \forall \theta,s,\st_f,\st_\delta\ldotp \\
\quad \theta,s,\st_f\models \sst_f \land \theta, s, \st_\delta \modelsp \corepred{\delta}{\sins}{\souts} \land \st_f\disj\st_\delta \implies \\
\qquad \exists \st\ldotp \st = \st_\delta \cdot \st_f \land \theta,s,\st\models \sst \land \SAT(\pc)
\end{array}
\end{equation}

\begin{equation}
\tag{Consume OX: No Path Drops}\label{eq:consume-ox-no-drop}
\begin{array}{l}
(\forall o, \sst_f,\souts,\pc \ldotp \ppcons{\OX,\sst,\delta,\sins}{o, \sst_f, \souts, \pc}\Rarr o_c = \Ok) \implies \\
\quad \exists  \sst_f',\souts',\pc' \ldotp \ppcons{\OX, \sst, \delta,\ins,\pc}{\Ok, \sst_f', \souts', \pc'}
\end{array}
\end{equation}

\begin{equation}
\tag{Consume UX Soundness}\label{eq:consume-ux-sound}
\begin{array}{l}
\ppcons{\sst,\delta,\sins}{\Ok,\sst_f,\souts,\pc} \implies \forall \theta,s,\st\ldotp \\
\quad  \theta,s,\st\models \sst \land \SAT(\pc) \implies \exists \st_\delta, \st_f\ldotp\\
\qquad \st_\delta\disj\st_f \land \st=\st_\delta\cdot\st_f \land \theta, s, \st_\delta \modelsp \corepred{\delta}{\sins}{\souts} \land \theta,s,\st_f\models \sst_f
\end{array}
\end{equation}

The above three axioms show the soundness of \consume{} for OX and UX operations. They are similar to \ref{eq:mem-ox-soundness} and \ref{eq:mem-ux-soundness} respectively. We also have an additional requirement for OX execution, stating that if there is no erroneous execution of \consume{} then at least one successful execution exists.

\begin{equation}
\tag{Produce: OX Soundness}\label{eq:produce-ox-sound}
\begin{array}{l}
\theta, s, \st_f \models \sst_f \land \theta, s, \st_\delta \modelsp \corepred{\delta}{\sins}{\outs} \land \st_f\disj\st_\delta \implies \\
\quad \exists \sst\ldotp \ppprod{\sst_f,\delta,\sins,\souts}{\sst,\pc} \land \SAT(\pc) \land \theta,s,(\st_f\cdot\st_\delta) \models \sst
\end{array}
\end{equation}

\begin{equation}
\tag{Produce: UX Soundness}\label{eq:produce-ux-sound}
\begin{array}{l}
\ppprod{\sst_f,\delta,\sins,\souts}{\sst, \pc} \implies\\
\quad \forall\theta,s,\st\ldotp \SAT(\pc) \land \theta,s,\st\models\sst \implies \exists \st_\delta, \st_f\ldotp \\
\qquad \st_\delta\disj\st_f \land \st=\st_\delta\cdot\st_f \land \theta,s,\st_\delta \modelsp \corepred{\delta}{\sins}{\souts} \land \theta,s,\st_f \models \sst_f
\end{array}
\end{equation}

Again, these two axioms show soundness of \produce{} for OX and UX operation. For the former, we show that if two disjoint states exist in the concrete world and one of the two satisfies a core predicate, then producing that predicate onto the other yields their composition. For the latter, we state that if \produce{} yields a branch that is satisfiable then there must exist a model of the result.

There are no axioms that \fix{} needs to satisfy -- as it simply results in assertions, it cannot break soundness. Either it produces no fixes or only incompatible fixes and we vanish, which is UX sound, or it finds fixes that work, in which case we already know the execution exists in the concrete world and we remain UX sound.

\section{State Models} \label{sec:theory-state-models}

Now that the CSE engine is defined and the properties of state models are axiomatised, we focus on state models that can be later used to build up complex state. We first look at the base case, \Ex{}, followed by the state model of knowledge or agreement, \Ag. Finally, we look at an in-between, \Frac, that allows exclusive ownership with a form of sharing.

\subsection{Exclusive}

The exclusive state model $\Ex(X)$ presented earlier is the simplest form of state model. It asserts ownership of an element of the set $X$, provides one core predicate $\exP$, and two actions to modify the state, $\load$ and $\store$. The $\exP$ predicate has no ins, and one out: the value stored.
\begin{align*}
 	\Ex(X) &\defeq	\ex{x\colon X}\\
 	|\ex{x}| &\defeq \bot\\
 	\ex{x_1} \cdot \ex{x_2}&~\text{is always undefined}
\end{align*}

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[ExStoreOk]{}{\ppexec{\store}{\ex{\sym x}, [\sym x']}{\Ok, \ex{\sym x'}, [], []}}
\and\inferrule[ExStoreMiss]{}{\ppexec{\store}{\bot,[\sym x']}{\Miss,\bot,[],[]}}
\and\inferrule[ExConsOk]{}{\ppcons{\ex{\sym x}, \exP, []}{\Ok, \bot, [\sym x], []}}
\and\inferrule[ExProd]{}{\ppprod{\bot,\exP, [], [\sym x]}{\ex{\sym x}, []}}
\and\inferrule[ExFix]{}{\fix ~[] = [\{ \exists \sym x\ldotp \corepred{\exP}{}{\sym x} \}]}
	\end{mathpar}
	\caption{Some rules for \Ex}
	\label{fig:ex-rules-example}
\end{figure}

The first notation, $\Ex(X)$ is the state model instantiation from the set $X$ to the $\Ex$ resource algebra. The notation $\ex{x\colon X}$ stands for $\{\ex{x} : \forall x\in X \}$ -- here ``$\ex{x}$'' refers to the particular element of the $\Ex(X)$ RA where the value is $x\in X$.

See \autoref{fig:ex-rules-example} for an extract of the rules for \Ex{} -- the full definition can be found in \autoref{rules:ex}. We also show side by side the rules for the concrete and symbolic actions in \autoref{fig:ex-rules-con-vs-sym}: in particular we note that they are strikingly similar, the latter simply being lifted from the former.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[CExStoreOk]{}{\ppexecc{\store}{\ex{x}, [x']}{\Ok, \ex{x'}, []}}
\and\inferrule[ExStoreOk]{}{\ppexec{\store}{\ex{\sym x}, [\sym x']}{\Ok, \ex{\sym x'}, [], []}}
\and\inferrule[CExStoreMiss]{}{\ppexecc{\store}{\bot,[x']}{\Miss,\bot,[]}}
\and\inferrule[ExStoreMiss]{}{\ppexec{\store}{\bot,[\sym x']}{\Miss,\bot,[],[]}}
	\end{mathpar}
	\caption{Side by side comparison of concrete and symbolic action rules for \Ex{} -- concrete rules are prefixed with \textsc{C}.}
	\label{fig:ex-rules-con-vs-sym}
\end{figure}

The only difference for \Ex{} between RAs and PCMs is the absence of a $0$, which is instead $\bot$; in fact $\Ex(X)^?$ is strictly equivalent to $\Ex_\PCM(X)$.

To provide at least one full of example of state model, we also include the definition of the symbolic interpretation and core predicate satisfiability relations -- both are trivial, simply checking for equality:
\begin{mathpar}
\inferrule[ExSymInterpretation]{\expeval{\sym x}=x}{\theta,s,\ex{x}\models\ex{\sym x}}
\and\inferrule[ExPredSat]{\st=\ex{x}}{\st\modelsp\corepred{\exP}{[]}{[x]}}
\end{mathpar}

\subsection{Agreement}

The agreement state model $\Ag(X)$ is the counterpart to the exclusively owned state model -- it allows sharing, and is duplicable. It defines one predicate, $\agP$, and a single action, $\load$. Again, $\agP$ has no ins and one out, the value. Mutating state is not sound with a duplicable resource, as that doesn't satisfy \ref{eq:frame-sub} or \ref{eq:frame-add}: a frame that is compatible with the original value is not compatible with the modified value, as they are not equal any more.
\begin{align*}
	\Ag(X) &\defeq \ag{x\colon X}\\
	|\ag{x}| &\defeq \ag{x}\\
	\ag{x} \cdot \ag{x'} &\defeq \begin{cases}
 	\ag{x} &\If~x = x' \\
 	\text{undefined} &\Otherwise
 \end{cases}
\end{align*}

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[AgLoadOk]{}{\ppexec{\load}{\ag{\sym x},[]}{\Ok,\ag{\sym x},[\sym x],[]}}
\and\inferrule[AgConsOk]{}{\ppcons{\ag{\sym x},\agP,[]}{\Ok,\ag{\sym x},[\sym x],[]}}
\and\inferrule[AgProdBot]{}{\ppprod{\bot,\agP,[],[\sym x]}{\ag{\sym x}, []}}
\and\inferrule[AgProdEq]{}{\ppprod{\ag{\sym x},\agP,[],[\sym x']}{\ag{\sym x}, [\sym x = \sym x']}}
	\end{mathpar}
	\caption{Some rules for \Ag}
	\label{fig:ag-rules-example}
\end{figure}

See \autoref{fig:ag-rules-example} for an extract of its rules. Its duplicable nature can be seen in the rules \ruleref{AgConsOk} and \ruleref{AgProdEq}: consuming the predicate doesn't turn the state into $\bot$ but leaves it untouched, while producing into an already present state succeeds as long as the predicate's out and the state are equal, as seen in the path condition: $[\sym x = \sym x']$. An observation that can be made from this is that it is always sound for consumption of a state to result in \emph{at least} the input's core; as this knowledge is duplicable, it can always be kept anyway.

Unlike \Ex, \Ag{} is \emph{not cancellative}: in other words, $a\cdot b = a\cdot c \nRightarrow b=c$, since from the definition of composition we have $\ag{x} \cdot \bot=\ag{x}\cdot\ag{x}$ and $\bot\neq\ag{x}$. Cancellativity is a property that was initially thought to be required for separation algebras \cite{abstractseplogic,sepalgebra}, but that has since been dropped~\cite{statesoundness,iris}.

The agreement state model is thus useful to model immutable, shareable state -- for instance, it is used in the JavaScript instantiation (discussed in \cref{sec:impl-instantiations}) to represent the address of an object's metadata, as this information is immutable. This was first brought to a CSE engine in JaVerT \cite{javert1,javert2}, though when that was developed the notion of an ``agreement state model'' didn't exist and as such it wasn't identified as one.

\subsection{Fraction}

The fractional state model $\Frac(X)$ allows for exclusive ownership while allowing sharing, via fractional permissions. This idea originates from \cite{fracpermissions,fracpermissions2}, where the ``points to'' assertion is equipped with a fraction $q$: a state satisfying $a\mapstoq{1}x$ has full ownership (read and write) of $x$, and can be split into $a\mapstoq{q}x * a\mapstoq{1-q}x$, creating two states with read-only permissions. The \Frac{} memory model is a generalisation of this at the value level. It defines one predicate, $\fracP$, that has one in, the fraction $q$, and one out, the value $x$. Just like $\Ex$, it provides a \load{} and a \store{} action -- the only difference being that a fraction of $1$ is required to modify the value. Because the maximum fraction is $1$, we know that when it is $1$ no frame can have ownership of the value, and it can thus safely be modified.
\begin{align*}
	\Frac(X)&\defeq \fracc{x\colon X, q\colon (0;1]}\\
	|\fracc{x, q}| &\defeq \bot\\
	\fracc{x,q}\cdot\fracc{x',q'}&\defeq \begin{cases}
		\fracc{x,q+q'} & \If~ x=x' \land q+q' \leq 1\\
		\text{undefined} &\Otherwise
	\end{cases}
\end{align*}

We put the fraction $q$ in the in-values of the $\fracP$ core predicate to allow \consume{} to subtract exactly the permission required from the state -- if both the permission and the value were out-values, then \consume{} would always evaluate to $\bot$, cancelling the main advantage of this state model. The rules for \consume{} can be seen in \autoref{fig:frac-consume-rules}. We note here that in the symbolic state model, the fraction part is also lifted to the symbolic realm, allowing for a symbolic permission.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[FracConsAll]{}{\ppcons{\fracc{\sym x,\sym q},\fracP,[\sym q']}{\Ok,\bot,[\sym x],[\sym q=\sym q']}}
\and\inferrule[FracConsSome]{}{\ppcons{\fracc{\sym x,\sym q},\fracP,[\sym q']}{\Ok,\fracc{\sym x,\sym q-\sym q'},[\sym x],[0 < \sym q' < \sym q]}}
\and\inferrule[FracConsMiss]{}{\ppcons{\fracc{\sym x,\sym q},\fracP,[\sym q']}{\Miss,\fracc{\sym x,\sym q},[\sym q' - \sym q],[\sym q < \sym q' \leq 1]}}
\and\inferrule[FracConsFail]{}{\ppcons{\fracc{\sym x,\sym q},\fracP,[\sym q']}{\LFail,\fracc{\sym x,\sym q},[],[\sym q' \leq 0 \lor 1 < \sym q']}}
	\end{mathpar}
	\caption{\consume{} rules for \Frac}
	\label{fig:frac-consume-rules}
\end{figure}

Because the outcome of actions depends on the amount of permission owned, \Frac{} is also a good example to show how one may adapt a concrete state model into a symbolic state model, by moving the relevant concrete conditions into the path condition. The rules for the concrete and symbolic \store{} are shown in \autoref{fig:frac-store-rules}. Note how, between \ruleref{CFracStorePerm} and \ruleref{FracStorePerm}, the condition $q < 1$ is replaced by $\sym q < 1$, and is returned in the path condition, rather than being checked for in the rule itself, since the value of $\sym q$ depends on the current PC.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[CFracStoreOk]{q=1}{\ppexecc{\store}{\fracc{x,q},[x']}{\Ok,\fracc{x',q},[]}}
\and\inferrule[CFracStorePerm]{q<1}{\ppexecc{\store}{\fracc{x,q},[x']}{\Miss,\fracc{x,q},[1-q]}}
\and\inferrule[FracStoreOk]{}{\ppexec{\store}{\fracc{\sym x,\sym q},[\sym x']}{\Ok,\fracc{\sym x',\sym q},[],[\sym q=1]}}
\and\inferrule[FracStorePerm]{}{\ppexec{\store}{\fracc{\sym x,\sym q},[\sym x']}{\Miss,\fracc{\sym x,\sym q},[1-\sym q],[\sym q<1]}}
	\end{mathpar}
	\caption{\store{} rules for \Frac}
	\label{fig:frac-store-rules}
\end{figure}

While this state model is conceived taking into account fractional permissions, other separation algebras exist to tackle the same problem. For instance, \cite{fracpermissions} also proposes a counting permissions system: $a\mapstoq{0}x$ is a writing permission, and it can issue reading permissions denoted $a\part x$, such that $a\mapstoq{q}x \iff a\mapstoq{q+1}x * a\part x$. Though we do not define it in full, this could also be implemented as a state model, providing two core predicates: \corepred{\code{write}}{q}{x} and \corepred{\code{read}}{}{x}, with every consumption of \code{read} from a writeable state increasing $q$, and inversely every production of \code{read} decreasing it, and checking $q=0$ when using \store. This shows state models seem to be the right abstraction, as they support a variety of resource algebras.

\section{State Model Transformers} \label{sec:theory-state-model-transf}

While the above state models can prove useful, they are limited in use, only allowing the storage of one value. One would likely want to model state that is more complex than this, storing a range of values in a variety of ways. Iris introduces the idea of RA constructions \cite{iris} -- this idea has then been adapted to Gillian in \cite{sacha-phd}, with PCMs. Here we revisit these constructions, porting them back to RAs. We call them \emph{state model transformers},\footnote{Not to be confused with ``state monad transformers'', which allow adding state to a monad.} or state transformers.

\subsection{Sum}

The sum state model $\Sum(\mmdl_1,\mmdl_2)$, also written $\mmdl_1 + \mmdl_2$, allows representing states that can be in either the states of $\mmdl_1$ or $\mmdl_2$. This state model doesn't introduce any core predicates or actions, and instead simply lifts those of the two underlying state models. Let $\actions_1$ and $\preds_1$ the actions and core predicates of $\mmdl_1$, and $\actions_2$ and $\preds_2$ those of $\mmdl_2$, the actions of the sum are defined as ${\actions = \{\code{L}\alpha : \alpha \in \actions_1 \} \uplus \{\code{R}\alpha : \alpha \in \actions_2 \}}$, and the core predicates as ${\preds = \{\code{L}\delta : \delta \in \preds_1 \} \uplus \{\code{R}\delta : \delta \in \preds_2 \}}$. Actions and predicates are simply tagged with what side of the sum they originate from.

Let $\St_1$ and $\St_2$ be the carrier sets of the RAs of $\mmdl_1$ and $\mmdl_2$ respectively; the RA of $\Sum(\mmdl_1,\mmdl_2)$ is:
\begin{align*}
	\Sum(\mmdl_1, \mmdl_2) \defeq \mmdl_1+\mmdl_2 &\defeq l(\st\colon \St_1) ~|~ r(\st\colon \St_2)\\
	l(\st)\cdot l(\st') &\defeq l(\st\cdot \st')\\
	r(\st)\cdot r(\st') &\defeq r(\st\cdot \st')\\
	|l(\st)|&\defeq \begin{cases}
 		\bot &\If~ |\st|=\bot\\
 		l(|\st|) &\Otherwise
	 \end{cases}\\
	 |r(\st)|&\defeq \begin{cases}
 		\bot &\If~ |\st|=\bot\\
 		r(|\st|) &\Otherwise
	 \end{cases}
\end{align*}

In \cref{sec:unsoundness-in-sum} we showed how $\Sum_\PCM$ must disallow the unit on each side to be sound. In RAs however, this is not an issue any more, as the underlying state models can simply not define a unit. We may also redefine \isexowned{} to take into account that the unit is not present in the carrier set, giving a much simpler definition: \begin{align*}
	\isexowned~\st \defeq \nexists \st'\in\St\ldotp \st\disj\st'
\end{align*}

The rules of the \Sum{} state model are otherwise straightforward, lifting \execac, \consume{} and \produce{} to the sum from the underlying state models. The only minor difference is when a \Miss{} occurs -- because fixes are only generated from a $\lvallist$, \Sum{} has no way of knowing which side raised the \Miss, and can thus not call the \fix{} function of the correct side. To avoid this, when the outcome of \execac{} or \consume{} is a \Miss{} we concatenate an identifier ($\strv{l}$ or $\strv{r}$) to the values to allow discriminating between the two. An extract of these rules is shown in \autoref{fig:sum-miss-example}.

\begin{figure}
	\centering
\begin{align*}
	\text{Given }
	\fn{wrap}_l(x) = \begin{cases}
		\bot &\If~x=\bot\\
	 	l(x)&\Otherwise
	 \end{cases} \text{ and }
	 \fn{unwrap}_l(x_l) = \begin{cases}
 		\bot &\If ~x=\bot\\
 		x &\If~x_l=l(x)\\
 		\text{undefined}&\Otherwise
	 \end{cases}
\end{align*}
	\begin{mathpar}
\inferrule[SumLAction]{\sst=\fn{unwrap}_l(\sst_l) \\ \ppexec{\alpha}{\sst,\sins}{o,\sst',\souts,\pc} \\ \sst_l' = \fn{wrap}_l(\sst') \\ o \neq \Miss}{\ppexec{\code{L}\alpha}{\sst_l,\sins}{o,\sst_l',\souts,\pc}}
\and\inferrule[SumLActionMiss]{\sst=\fn{unwrap}_l(\sst_l) \\ \ppexec{\alpha}{\sst,\sins}{\Miss,\sst',\souts,\pc} \\ \sst_l' = \fn{wrap}_l(\sst')}{\ppexec{\code{L}\alpha}{\sst_l,\sins}{\Miss,\sst_l',\strv{l}::\souts,\pc}}
\and\inferrule[SumLFix]{\mmdl_1.\fix~\sins=a}{\fix~\strv{l}::\sins=a}
	\end{mathpar}
	\caption{Rules for handling \Miss{} in \Sum}
	\label{fig:sum-miss-example}
\end{figure}

This example is interesting in that it also shows the constraint imposed by the fact $\bot\notin\Sst$: it is often needed to define auxiliary $\fn{unwrap}$ and $\fn{wrap}$ functions, that get the state out a value if it is not $\bot$, and returns $\bot$ otherwise. In other words, $\fn{unwrap}\colon\Sst_1^?\part\Sst_2^?$ and $\fn{wrap}\colon\Sst_2^?\rarr\Sst_1^?$.\footnote{Note how $\fn{unwrap}$ is a partial function; for instance in the case of a sum, attempting to unwrap a state from the left side into the right side is undefined.} While these are straightforward to define and use, they can clutter rules a bit, in favour of not needing to handle $\bot$ and non-$\bot$ cases separately.

Compared to $\Sum_\PCM$, we now need two less properties to guarantee a sound transition from one side of the sum to the other: indeed, we don't need to exclude the unit of either sides from the carrier set, since $\bot\notin \St_1$ and $\bot\notin\St_2$ already, by definition.

\subsection{Product}

The product state model $\Product(\mmdl_1, \mmdl_2)$, also written $\mmdl_1 \times \mmdl_2$, allows representing pairs of states, where each side belongs to a specific state model. For instance, one could chose to represent the $\Frac(X)$ state model as $\Ex(X) \times \Frac_{\mathbb{Q}}$, where $\Frac_{\mathbb{Q}}$ is an exclusively owned rational in $(0;1]$, and with composition defined as addition. This is, in fact, the way Iris defines $\Frac$ \cite{iris-thesis} -- however, this approach makes using the state model less practical, as one would need to define a separate predicate for each side.

Iris also defines the product RA, by simply lifting pointwise the operations on each side of the product. We call this initial definition $\Product_0$, defining it as: \begin{align*}
 	\Product_0(\mmdl_1,\mmdl_2)&\defeq \St_1 \times \St_2\\
 	(\st_l,\st_r)\cdot(\st_l',\st_r') &\defeq (\st_l\cdot \st_l',\st_r\cdot \st_r')\\
 	|(\st_l,\st_r)|&\defeq \begin{cases}
(|\st_l|, |\st_r|) &\If~|\st_l|\neq\bot \land |\st_r|\neq\bot\\
\bot&\Otherwise
\end{cases}
\end{align*}

This definition, while straightforward, has a problem: while its sets of states is $\St_1\times\St_2$, we still have that $\bot\notin\St$. If an empty state ($\bot$) produces a core predicate for one of its sides, what should happen to the other side? While one side becomes defined, such that $\st_l\in\St_1$, $(\st_l, \bot)$ is not a valid state, since $\bot \notin \St_2$. This is a problem, since in our CSE engine all states are initialised as $\bot$ and can only be built upon via actions or predicate production, one predicate at a time. It seems here that the consume-produce interface of our engine, which allows creating state bit by bit, can be unadapted for certain RAs. Of course one can define additional core predicates for a specific product instantiation that combines predicates of underlying state models, however this goes against the core idea of state model transformers to minimise the amount of effort needed to construct new working state models. We also note that this problem is not encountered in $\Product_\PCM$, since its unit could be defined as $(0_1, 0_2)$ and work straightforwardly.

We define an alternative \Product{} RA that is more suited to this engine: \begin{align*}
	\Product(\mmdl_1,\mmdl_2) \defeq \mmdl_1 \times \mmdl_2 &\defeq (\St_1^? \times \St_2^?) \setminus \{(\bot, \bot)\}\\
	(\st_l, \st_r) \cdot (\st_l', \st_r') &\defeq (\st_l \cdot \st_l', \st_r \cdot \st_r')\\
	|(\st_l, \st_r)| &\defeq \begin{cases}
		\bot &\If~|\st_l|=\bot\land |\st_r|=\bot\\
		(|\st_l|, |\st_r|) &\Otherwise
 	\end{cases}
\end{align*}

We take advantage of the option state model $-^?$ and use that instead, thus allowing either sides of the product to be $\bot$. Crucially however, we exclude the $(\bot, \bot)$ state and ensure in the $\fn{wrap}$ helper that it never occurs (see \autoref{fig:product-rules-example}). This is important, because it means \emph{exclusivity} is carried: if for $(\st_l,\st_r)$ we have ${\isexowned~\st_l \land \isexowned~\st_r}$, then it also follows that $\isexowned~(\st_l, \st_r)$. If we hadn't excluded $(\bot, \bot)$, then it would always be the case that $(\st_l,\st_r)\cdot(\bot,\bot)$ holds, meaning that the state could never be exclusively owned. This is unpractical, as it would mean that disposing of a product is not frame preserving.

\begin{figure}
	\centering
\begin{align*}
	\text{Given }
	\fn{wrap}(x, y) = \begin{cases}
		\bot &\If~x=\bot \land y=\bot\\
		(x,y) &\Otherwise
	 \end{cases} \text{ and }
	 \fn{unwrap}(s) = \begin{cases}
 		(\bot, \bot) &\If ~s=\bot\\
		(x, y) &\Otherwise
	 \end{cases}
\end{align*}
	\begin{mathpar}
\inferrule[ProductLAction]{(\sst_l,\sst_r)=\fn{unwrap}(\sst) \\ \ppexec{\alpha}{\sst_l,\sins}{o,\sst_l',\souts,\pc} \\ \sst' = \fn{wrap}(\sst_l',\sst_r) \\ o \neq \Miss}{\ppexec{\code{L}\alpha}{\sst,\sins}{o,\sst',\souts,\pc}}
\end{mathpar}
	\caption{Example of action rules in \Product}
	\label{fig:product-rules-example}
\end{figure}

\subsection{Freeable}

The $\Freeable(\mmdl)$ state model transformer allows extending a state model with a \free{} action, that allows freeing a part of memory. The freed memory can then not be accessed, and attempting to use it raises a use-after-free error. The notion of marking freed memory as freed, rather than simply discarding it, comes from Incorrectness Separation Logic~\cite{isl}, as indeed simply disposing of the memory would break \ref{eq:frame-add}.

\Freeable{} is similar to the \statemodel{OneShot} RA of Iris, with the key difference being that the \freedP{} predicate used to mark a resource as freed is \emph{not duplicable}. Iris defines $\statemodel{OneShot}(X) \defeq \Ex(\{1\}) + \Ag(X)$ \cite{iris}, which is not UX-sound; in order for the switch of the side of a sum to be UX-sound, the resulting state (here, $\freed$) must be exclusively owned, which $\Ag$ never is. This is not a problem for Iris, that is only concerned with OX soundness, but because our goal is to make our state models both OX and UX sound for additional flexibility, we need to make this change.

To ensure frame preservation, the underlying state model must provide an additional function, equivalent to the aforementioned property ``exclusive'': ${\isexowned\colon\St\rarr\bools}$. Because actions in the symbolic realm need to be sound with respect to the concrete realm, we then lift its definition to ${\isexowned\colon\Sst\rarr\LVal}$, making it return a symbolic value that can be appended to the path condition.

While not needed for the core and compositional engines, the bi-abductive engine requires that  misses may be fixed; $\Freeable(\mmdl)$ however cannot access directly the core predicates of \mmdl{} to provide fixes when freeing an empty state. As such, the state model must also provide a ${\code{fix\_owned}\colon\Sst^?\rarr\lvallist}$ function that returns possible fixes to make the given state exclusively owned. This also implies that for any state $\st$ of \mmdl{}, if it is not exclusively owned then there must exist a bigger state $\st'$ such that $\st\preo\st'$ and $\isexowned~\st'$. This is not the case, for instance, for \Ag, meaning a construction such as $\Freeable(\Ag(X))$ is not sound.

Like Iris, we define the $\Freeable(\mmdl)$ state model by construction: ${\Freeable(\mmdl) \defeq \mmdl + \Ex(\{\freed\})}$, which allows all associated rules to be kept. For clarity, we rename the core predicate $\code{R}~\exP$ (defined by~$\Ex(\{\freed\})$) as $\freedP$. We also extend its actions with the $\free$ action.

Because \Freeable{} is constructed via other state model transformers, we only need to describe the rules for \free{}, which are shown in \autoref{fig:freeable-free-rules} -- the rest of the construction is already defined. This shows how simpler state models can be extended while alleviating the user from the burden of proving the soundness of the base construction.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[FreeableActionFree]{}{\ppexec{\free}{l(\sst),[]}{\Ok,r(\ex\freed),[],[\isexowned~\sst]}}
\and\inferrule[FreeableActionFreeErr]{}{\ppexec{\free}{l(\sst),[]}{\Miss,l(\sst),\code{fix\_owned}~\sst,[\neg\isexowned~\sst]}}
\and\inferrule[FreeableActionFreeMiss]{}{\ppexec{\free}{\bot,[]}{\Miss,\bot,\code{fix\_owned}~\bot,[]}}
\and\inferrule[FreeableActionDoubleFree]{}{\ppexec{\free}{r(\ex\freed),[]}{\Err,r(\ex\freed),[],[]}}
\end{mathpar}
	\caption{Symbolic action rule of \Freeable}
	\label{fig:freeable-free-rules}
\end{figure}

Conveniently, use-after-free errors are already handled by the sum construction, thanks to the rule \ruleref{SumLActionIncompat} which forbids actions from one side of the sum to be executed on the other side.

\subsection{Partial Map}

The partial map state model transformer $\PMap(I,\mmdl)$ is one of the most important state model transformers, as it allows having multiple state instances at different addresses and accessing them. It is constructed from a domain, $I\subseteq\Val$, and a state model for the codomain. For instance, the traditional SL linear heap can be modelled as $\PMap(\nats, \Ex(\Val))$. The C, JavaScript and Rust memory models can also all be modelled using a $\PMap$ \cite{sacha-phd}, despite them being radically different languages.

It defines one action, \alloc{}, and one core predicate, \domainset. It also \emph{lifts} all actions and predicates of the underlying model, adding to them an index argument or in-value. For instance, while for $\Ex(X)$ one has the core predicate $\corepred{\exP}{}{x}$, for $\PMap(\Ex(X))$ one would instead have predicates of the form $\corepred{\exP}{i}{x}$, which corresponds to the ``points to'' assertion $i\mapsto x$. We define it's RA as: \begin{breakalign*}
	\PMap(I,\mmdl) &\defeq (I \finmap \mmdl.\Sigma) \times \pset(I)^?\\
	(h,d)\cdot (h',d') &\defeq (h'', d'') \\
	\text{where } h''&\defeq \lambda i.\begin{cases}
		h(i)\cdot h'(i)&\If~i\in \dom(h)\cap\dom(h')\\
		h(i) &\If~i\in \dom(h) \setminus \dom(h')\\
		h'(i) &\If~i\in \dom(h') \setminus \dom(h)\\
		\text{undefined}&\Otherwise
	\end{cases}\\
	\text{and }d''&\defeq\begin{cases}
		d&\If~d'=\bot\\
		d'&\If~d=\bot\\
		\text{undefined}&\Otherwise
	\end{cases}\\
	\text{and }& d'' = \bot \lor \dom(h'')\subseteq d''\\
	|(h, d)| &\defeq \begin{cases}
		\bot &\If~\dom(h')=\emptyset\\
		(h', \bot) &\Otherwise
	\end{cases}\\
	\text{where }h'&\defeq \lambda i.\begin{cases}
		|h(i)| &\If~ i\in\dom(h) \land |h(i)| \neq \bot\\
		\text{undefined} &\Otherwise
	\end{cases}
\end{breakalign*}

An element of \PMap{} is of the form $(h, d)$, where $h$ are the mappings from locations to states, and $d$ is the \emph{domain set}, the set of indices that are known to exist. It is defined such that $\dom(h)\subseteq d$ always holds. The domain set can also be $\bot$, in which case there is no knowledge on what addresses exist or don't. The aim of the domain set is to allow separating invalid accesses from misses: if $d\neq\bot$, then any access to $i\notin d$ leads to an $\Err$ outcome, however if $i\in d$ and the binding does not exist in the heap, then a $\Miss$ is raised.

As a consequence of this, $\PMap(I,\mmdl)$ has an additional requirement for $\mmdl$: any action execution on $\bot$ must lead to a $\Miss$. \begin{align*}
	\ppexec{\alpha}{\bot,\ins}{o,\st',\outs,\pc} \implies o=\Miss
\end{align*}

This is needed for frame preservation; if this wasn't the case, one could run into a case where the action of a cell in the heap succeeds on an empty heap, but an empty domain set would be compatible with the state, despite its composition with the state leading to a different outcome (an \Err).

This constraint also comes from the fact that \PMap{} can't automatically yield a \Miss{} when executing actions on missing cells, and must instead execute the action on $\bot$. Indeed, because $\PMap(I,\mmdl)$ can accept any underlying state model, it does not ``know'' what values are needed to fix the missing cell. Instead, it needs $\mmdl$ to raise a $\Miss$ that it can then lift with an index.

As mentioned before, \PMap{} lifts actions and core predicates, adding to them an index. Getting the state associated to an index is not simple, and will in fact be the target of several optimisations in \cref{sec:theory-optim-pmap}. For now we consider the simplest form of symbolic matching: either the index is present in the heap, or it isn't -- if it isn't and the domain set is not $\bot$, we also know that it must be in the domain set. This creates three rules one must consider. For instance, given a state $([\sym a\mapsto \sym x, \sym b \mapsto \sym y], \bot)$, when an action is executed with index $\sym c$, three branches are created: either $\sym a = \sym c$, or $\sym b = \sym c$, or $\sym c$ is not part of the current heap, giving us $\sym c\notin \{\sym a,\sym b\}$.

Because most \execac, \consume{} and \produce{} rules of \PMap{} require retrieving states at a specific index, we introduce an abstraction over this: the \code{get} and \code{set} functions. The former allows getting the state at an index \emph{with branching}: $\code{get}\colon \PMap(I,\mmdl)^? \rarr I \rarr \pset(I \times \St^? \times \Pc)$. It returns the index to modify exactly the returned state, the state itself, and a path condition corresponding to the branch. The $\code{set}$ function has signature $\code{set}\colon \PMap(I,\mmdl)^?\rarr I \rarr \St^? \rarr \PMap(I,\mmdl)^?$. Unlike for \code{get}, we don't allow branching when setting: because branching already happened when getting, and \code{get} returns the index to use to modify the state, we can use this new index and be guaranteed that it is sound. For instance for the above example, the three executions are ${\ppget{\sst,\sym c}{\sym a, \sst_a, [\sym c = \sym a]}}$, ${\ppget{\sst, \sym c}{\sym b, \sst_b, [\sym c = \sym b]}}$ and ${\ppget{\sst,\sym c}{\sym c, \bot, [\sym c\notin \{\sym a,\sym b\}]}}$. We present the rules for \code{get} in \autoref{fig:pmap-get-rules}.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[PMapGetMatch]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym i'\in\dom(\sym h) \\ \sst_i=\sym h(\sym i')}{\ppget{\sst, \sym i}{\sym i', \sst_i, [\sym i=\sym i']}}
\and\inferrule[PMapGetAdd]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym i\notin\dom(\sym h) \\ \sym d\neq\bot}{\ppget{\sst, \sym i}{\sym i,\bot, [\sym i\notin\dom(\sym h) \land \sym i\in \sym d]}}
\and\inferrule[PMapGetBotDomain]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym i\notin \dom(\sym h) \\ \sym d=\bot}{\ppget{\sst,\sym i}{\sym i,\bot,[\sym i\notin\dom(\sym h)]}}
\end{mathpar}
	\caption{Rules for \code{get} for \PMap}
	\label{fig:pmap-get-rules}
\end{figure}

Another unseemingly complex aspect of \PMap{} is allocation, for which the rules are presented in \autoref{fig:pmap-alloc-rules}. To allow instantiating, the wrapped state model must provide an \code{instantiate} function defined as $\code{instantiate}\colon \Val \rarr \St$, such that given the arguments passed to $\alloc$, it returns a newly instantiated state. An initial definition of \PMap{} had \alloc{} always succeed; for instance, a $\bot$ state could allocate, resulting in $([\sym i\mapsto \st_i], \bot)$, with $\st_i$ the instantiated state. This however does not satisfy \ref{eq:frame-sub}: the frame $(\emptyset, \{\})$, made of an empty heap and the empty domain set, is compatible with the state before the allocation, but is not compatible with the state after, since the domain of the heap $\{\sym i\}$ is not a subset of $\{\}$. We must thus enforce ownership of the domain set when allocating. This is odd: in allocation-based languages like C, one can always allocate a new cell, without needing to ``own'' anything, as the allocator is global. With this definition of \alloc{} however, one can't allocate without it, which can be problematic: for instance in a multi-threaded setting, two threads cannot own the domain set, since it is exclusively owned (in order to allow modification).

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[PMapAlloc]{\sym d\neq \bot \\ \sym i = \code{fresh}~I \\ \sst_i=\code{instantiate}(\sins) \\ \sym h' = \sym h[\sym i\leftarrow \sst_i] \\ \sym d' = \sym d\uplus \{\sym i\}}{\ppexec{\alloc}{(\sym h,\sym d), \sins}{\Ok, (\sym h',\sym d'), [\sym i], [\sym i=\sym i]}}
\and\inferrule[PMapAllocMiss]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym d=\bot}{\ppexec{\alloc}{\sst,\sins}{\Miss,\sst,[\strv{domainset}],[]}}
\end{mathpar}
	\caption{Rules for \alloc{} for \PMap}
	\label{fig:pmap-alloc-rules}
\end{figure}

This is a weakness of this \PMap{} definition, as we require a more restrictive behaviour than what is usually admitted with allocation. This also has the unfortunate side effect that whenever a function needs to allocate, it must specify a \domainset{} core predicate in its pre and postcondition, which can become quite verbose and unpractical.\footnote{In fact, the instantiations of Gillian ignore the domain set altogether; this comes with a slight automation cost, as out of bounds accesses can never be detected.} Attempts have been made to preserve the domain set while allowing for more flexible allocation semantics, for instance by having the domain set be a duplicable resource that allows modification and defining its composition as set union. However these relaxations often had as a consequence that out of bounds accesses couldn't reliably be detected any more, as indeed more heap cells with a larger domain set could always be composed to yield a successful outcome. In fact, \emph{allocation seems to be a topic that is in essence hard to tackle with separation logic}; \cite{seplogic2,localreasoning} already mentioned some of these difficulties regarding freshness of the new index. Even when using sophisticated logics like Iris, works on verifying CompCert-C code doesn't differentiate misses from out of bounds: ``An \code{rmap} does not distinguish between an unallocated block and a block on which it holds no ownership'' \cite{iriscompcertc}. This difficulty stems from the fact that allocators are inherently non-deterministic (a program can't ``guess'' what address is allocated) and global (two threads allocating are guaranteed to yield different addresses).

A solution to the above problem is to simply get rid of the domain set, and define \PMap{} only as the mappings from indices to states. This works, however one loses the ability to detect out of bounds accesses entirely, as additional cells can always be composed with the state to yield a non-\Miss{} outcome. For our purposes we keep the definition using a domain set, as it provides better automation and is adapted to our needs.

Using RAs here again means the problem we faced in \cref{sec:theory-state-of-affairs} does not occur, because $\bot\notin\St$, the binding $i\mapsto\bot$ is not valid. This is handled in the definition of the \code{set} function, where if the set state is $\bot$ the binding is removed. Using RAs forces us to think about this case, as otherwise the definition is not valid.

\subsection{Partial Map Variations}

The behaviour of \PMap{} is not always adapted for the state one wants to model. Here we describe in a shorter form some state models that function similarly to \PMap.

\subsubsection{Dynamic Partial Map}

The \emph{dynamic} partial map, $\DynPMap(I,\mmdl)$, allows dynamically allocating missing cells when they're accessed. It is used, for instance, when modelling objects in JavaScript. In JavaScript, accessing a property that hasn't been set simply yields a default value, \code{undefined}, and any property of an object can be set directly without needing to allocate it first. This can be easily achieved by modifying \PMap, such that any out of bounds accesses have the effect of instantiating the state at that location, storing it, and executing the action against it. In other words, instead of using the domain set to distinguish between \Err{} and \Miss, we use it to distinguish between \Ok{} and \Miss{} \cite{sacha-phd}.

We also need to remove the \alloc{} action, since allocation has no reason to exist; the cell can be modified directly instead.

\subsubsection{List}

The list state model, $\List(\mmdl)$, allows storing a list of states up to a bound. Instead of having a domain set, we define it as having a bound $n$ such that all indices are in $[0;n($. It is useful when wanting to represent continuous blocks of memory of known size. We use it for the state model of WISL, which uses a simple block-offset memory model: each memory access requires an address of the block, and the offset within the block to read from.

Because the bound can also be $\bot$, it serves the exact same purpose as the domain set in \PMap{}: distinguishing out of bounds from misses.

\subsubsection{General Map}

Due to the strong similarity between $\PMap$ and $\List$, it is tempting to define a more general sort of map state model, that allows replicating both behaviours. The general map $\GMap$ allows this; constructed as $\GMap(I,\mmdl,\mmdl_D)$, it receives the same domain $I$ and codomain $\mmdl$ as before, but it also receives a \emph{discriminator} state model $\mmdl_D$, which represents the state which is capable of discriminating out of bounds accesses from missing accesses.

The discriminator must come equipped with an $\code{is\_within}\colon \St_D \rarr I \rarr \bools$ function that given an index returns whether or not the state is within bounds. Here, the input discriminator state is never $\bot$: if the discriminator is not known then a \Miss{} must be issued, since composing a state with a non-$\bot$ discriminator can change the outcome. \code{is\_within} must also be lifted to the symbolic realm, resulting in $\code{is\_within}\colon \Sst_D\rarr I\rarr \pset(\LVal)$.

Defining \PMap{} and \List{} with a \GMap{} would require using the discriminator state models $\Ex(\pset(I))$ and $\Ex(\nats)$ respectively, with the following $\code{is\_within}$ definitions: \begin{align*}
	\code{is\_within}_{\PMap}~d~i&\defeq i\in d\\
	\code{is\_within}_{\List}~n~i&\defeq 0 \leq i \land i < n
\end{align*}

For the above two examples it is also needed to exclude the $\store$ actions defined by the discriminator (via \Ex), as modifying the domain set of a partial map or the bound of a list is not frame preserving. In general, it is unsound to modify the discriminator directly without also modifying the map (like \alloc{} does): ``increasing'' it breaks \ref{eq:frame-add} (as states compatible with the new state aren't compatible with the old state), while ``decreasing'' it breaks \ref{eq:frame-sub} (for the inverse reason). Here increasing and decreasing refer to modifying the discriminator such that more or less indices are considered \emph{within} it.

We have thus defined a wide range of different state models and state model transformers, that can be easily combined to build up a state model. Most notably, while in our examples we only considered variations of the linear heap, there is no reason why we can't define more complex state models the same way. For instance, $\Freeable(\PMap(\nats, \Freeable(\Ex(\Val) \times \Ex(\Str)))) \times (\Ag(\Val) + \Frac(\nats))$ is a perfectly valid and sound construction, and we do not need to verify any of it despite its complexity; if each application of a state model transformer is valid, then the result is sound.

\section{Optimising Partial Maps} \label{sec:theory-optim-pmap}

Map-like state models are both widely used and of varied applications, as seen before. They are also quite complex, as they can lead to many branches, even when only a small number of the branches is actually feasible. Consider the \PMap{} state $([ \sym a_1\mapsto x_1, ..., \sym a_n\mapsto x_n ], \bot)$. Executing a lookup on it will yield $n+1$ branches: one for each possible match, and one for the case where the index is different from all others. Such a state also needs to carry the fact that all addresses are distinct; for $n$ addresses, this is $\frac{n(n+1)}{2}$ inequalities. While at the theory level this isn't an issue, as we only worry about the soundness of the actions when the path condition is satisfiable, this can have a significant performance impact on the implementation.

Optimisations exist to reduce the number of branches generated, and while they have been implemented successfully in Gillian, there currently exists no theoretical justification to them proving their soundness. Here, we \emph{theoretically} define how to optimise \PMap-like state models, how to prove the soundness of these optimisations, and explore three different optimisations that already exist in Gillian.

\subsection{Syntactic Checking}

The simplest optimisation done to lookups is \emph{syntactic} checking: before attempting to branch on every index on the map, we check if the symbolic index is already present in the map, in which case no branching is needed.

This only requires modifying the matching rule for the \code{get} function, adding $\sym i\notin \dom(\sym h)$ to the precondition, and adding a rule that directly gets the relevant state; both rules can be seen in \autoref{fig:syntactic-check-rules}.

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[SyntacticPMapGetMatch]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym i\in\dom(\sym h) \\ \sst_i=\sym h(\sym i)}{\ppget{\sst, \sym i}{\sym i, \sst_{i}, []}}
\and\inferrule[SyntacticPMapGetBranch]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ \sym i\notin\dom(\sym h) \\ \sym i'\in\dom(\sym h) \\ \sst_{i}=\sym h(\sym i')}{\ppget{\sst, \sym i}{\sym i', \sst_{i}, [\sym i=\sym i']}}
	\end{mathpar}
	\caption{Modified rules for syntactic matching}
	\label{fig:syntactic-check-rules}
\end{figure}

This optimisation is what justifies returning an index for every action in \PMap. As an example, take the state model $\PMap(\Ex(\Val))$ in the state $([\sym a_1\mapsto x_a, ..., \sym a_n\mapsto x_n])$. When executing $\code{load(}\sym b\code{)}$ with a fresh symbolic variable $\sym b$, we branch $n+1$ times, and return the value stored there \emph{along with the used index}, for instance returning $[\sym a_3, \sym x_3]$. The program is then free to use $\sym a_3$ instead of $\sym b$ for further accesses to that cell, and thanks to syntactic checking no branching will be needed. Without syntactic checking, every access invariably causes branching, even if the index is already present.

An observation is that thanks to our \code{get} and \code{set} abstractions over accessing states at a given index, we can restrict ourselves to only modify these two definitions and carry over the rest of the rules of \PMap. Because these optimisations are purely due to the symbolicness of the heap indices, the concrete state model is untouched and we can prove soundness with respect to the concrete \PMap. Furthermore, we define symbolic \code{get} \emph{axioms}, which further ease proving soundness of the optimisations: when proving soundness of $\PMap$, we only use these axioms, meaning that any optimisation that satisfies the axioms is also automatically sound.
\begin{equation}
\tag{Get OX Soundness}\label{eq:get-ox-sound}
\begin{array}{l}
\theta,s,\st \models \sst \land \ppgetc{\st, i}{\st_i} \land \expeval{\sym i}=i \implies \exists \sst_i, \sym i', \pc \ldotp \\
\quad \ppget{\sst,\sym i}{\sym i', \sst_i, \pc} \land \theta,s,\st_i\models\sst_i \land \expeval{\sym i'}=i \land \SAT(\pc)
\end{array}
\end{equation}
\begin{equation}
\tag{Get UX Soundness}\label{eq:get-ux-sound}
\begin{array}{l}
\ppget{\sst,\sym i}{\sym i', \sst_i, \pc} \land \expeval{\sym i}=\expeval{\sym i'}=i \land \SAT(\pc) \implies \\
\quad \forall \st\ldotp \theta,s,\st \models \sst \implies \exists \st_i \ldotp \ppgetc{\st, i}{\st_i} \land \theta,s,\st_i \models \sst_i
\end{array}
\end{equation}

They are quite simple: for the OX case, any \code{get} call that exists in the concrete world must exist in the symbolic world. For the UX soundness, if a symbolic \code{get} returns a satisfiable branch then there is also a matching state $\st_i$ in the concrete world.

We also define two additional axioms that we do not prove but that must hold for all \code{get} and \code{set} pairs:
\begin{equation}
\tag{Get-Set Forwards Soundness}\label{eq:getset-sound}
\begin{array}{l}
\ppget{\sst,\sym i}{\sym i', \sst_i} \land \ppset{\sst, \sym i', \sst_i'}{\sst'} \implies \\
\quad (\exists \st,\st_i'\ldotp \theta,s,\st\models\sst \land \theta,s,\st_i'\models\sst_i') \implies \\
\qquad \exists \st'\ldotp \theta,s,\st'\models\sst'
\end{array}
\end{equation}
\begin{equation}
\tag{Get-Set Backwards Soundness}\label{eq:getset-sound}
\begin{array}{l}
\ppget{\sst,\sym i}{\sym i', \sst_i} \land \ppset{\sst, \sym i', \sst_i'}{\sst'} \implies \\
\quad (\exists \st',\st_i\ldotp \theta,s,\st'\models\sst' \land \theta,s,\st_i\models\sst_i) \implies \\
\qquad \exists \st\ldotp \theta,s,\st\models\sst
\end{array}
\end{equation}

These are quite straightforward: the former states that if a state exists in the concrete world and is modified with a substate that also exists in the concrete world, then the result must exist too. The latter states the reverse: if a state after \code{set} exists in the concrete world, and we know that the substate that used to be at the modified location also exists in the concrete world, then the original state also must exist.

These two axioms simply follow from the fact that $\sym i'$ represents the location to directly modify the returned state, and that \code{set} only modifies the state at the given index, the rest being untouched. The axioms are used when proving OX and UX soundness respectively, as they each use either forwards or backwards reasoning.

\subsection{Split PMap}

The \emph{split} partial map, \SplitPMap, goes further to reduce branches: it split the heap into two, one side being the \emph{concrete} part, and the other the \emph{symbolic}. A concrete value is a value that contains no symbolic variables, such that it's evaluation is unaffected by substitutions: $\exists e\ldotp \forall \theta,s\ldotp \expeval{\sym e}=e$. We introduce the predicate $\code{is\_concrete}$ that is true if a value is concrete. The symbolicness of a binding in the heap is determined from the value (the substate) \emph{and the key}; a concrete key pointing to a symbolic substate, or a symbolic key pointing to a concrete substate, both go in the symbolic part of the heap. Similarly to values, a state is concrete if its interpretation is unaffected by substitutions: ${\exists \st\ldotp \forall \theta,s\ldotp \theta,s,\st\models \sst}$. \SplitPMap(I,\mmdl) thus requires the state model to implement an $\code{is\_concrete}_\mmdl\colon \Sst \rarr \bools$ function, which determines whether the entire state is concrete. For instance for the $\Ex(X)$ state model, we have ${\code{is\_concrete}_\Ex~\ex{\sym x}\defeq \code{is\_concrete}~\sym x}$.

This has two advantages; the first minor advantage is that when looking for syntactic matches in the heaps, if the address is symbolic then we only must check in the symbolic part of the heap, as symbolic keys are not permitted in the concrete part.\footnote{This doesn't hold for concrete keys, as it can be that the concrete key maps to a symbolic state, in which case it goes in the symbolic part of the heap.} The second major advantage of this technique is related to an implementation detail of some CSE engines:~\emph{substitution}.

Substitution is the process of replacing logical values in a state with a new value.\footnote{Here we define substitution in states; this is different to substitution in expressions -- also known as $\alpha$-conversion -- which cannot be avoided. State substitution usually uses expression substitution.} For instance when learning $\sym a_1 = \sym a_2$, the engine may decide to substitute all occurrences of $\sym a_2$ with $\sym a_1$ and remove the equality from the PC if it is sound to do so (if $\sym a_2$ never occurs again). Because this process applies to the entire state, it can lead to unfortunate consequences, such as needing to merge two state fragments that live at the same location. Reusing the above example, if the state contained $[\sym a_1 \mapsto \sst_1, \sym a_2\mapsto \sst_2]$, then the substituted state would be $[\sym a_1 \mapsto \sst_1 \cdot \sst_2]$. This is problematic, as composition is not defined for symbolic state; symbolic states are an arbitrary set, unlike concrete states which form an RA. Current implementations usually mimic the composition of the concrete counterpart, branching when relevant; this is however not justified in the theory presented in this project, as it is caused by an implementation quirk rather than a theoretical necessity. In fact, what causes this is also a ``defect'' from the implementation, as it would mean the engine didn't assert that two different locations are in fact different. This phenomenon exists in Gillian under the name substitutions \cite{gillian2}, and in Viper under the name consolidation \cite{viper-consolidation}.

Despite the fact it is only needed due to implementation details, we argue that optimisations are also something that is only needed because of an implementation, which justifies attempting to prove soundness of techniques that are caused by theoretically unsound solutions.

The second advantage of \SplitPMap{} relating to substitutions is that these can be entirely skipped for the concrete part of the heap, since we already know that it doesn't contain any symbolic variables. Substitutions are computationally expensive, as they require traversing the entire state and attempting substitutions on all values; for large states, this can take a significant amount of time.

In \autoref{fig:splitpmap-set-rules} we present the rules relating to \code{set} for \SplitPMap. In particular, when this requires modifying the binding for one side of the heap, we must delete it from the other, as it can be that the state was concrete and became symbolic, or inversely.

\begin{figure}
	\centering
\begin{mathpar}
\inferrule[SplitPMapSetSomeCon]{(\sym h_c,\sym h_s,\sym d)=\fn{unwrap}(\sst) \\ \sst_i\neq\bot \\ \code{is\_concrete}_\St~\sst_i \\ \code{is\_concrete}~\sym i \\ \sym h'_c=\sym h_c[\sym i\leftarrow \sst_i] \\ \sym h'_s=\sym h_s[\sym i\not\leftarrow] \\ \sst'=\fn{wrap}(\sym h'_c, \sym h'_s, \sym d)}{\ppset{\sst,\sym i,\sst_i}{\sst'}}
\and\inferrule[SplitPMapSetSomeSym]{(\sym h_c,\sym h_s,\sym d)=\fn{unwrap}(\sst) \\ \sym s_i\neq\bot \\ \neg(\code{is\_concrete}_\St~\sst_i \lor \code{is\_concrete}~\sym i) \\ \sym h'_c=\sym h_c[\sym i\not\leftarrow] \\ \sym h'_s=\sym h_s[\sym i\leftarrow \sst_i] \\ \sst'=\fn{wrap}(\sym h'_c, \sym h'_s, \sym d)}{\ppset{\sst,\sym i,\sst_i}{\sst'}}
\and\inferrule[SplitPMapSetNone]{(\sym h_c,\sym h_s,\sym d)=\fn{unwrap}(\sst) \\ \sst_i=\bot \\ \sym h'_c=\sym h_c[\sym i\not\leftarrow] \\ \sym h'_s=\sym h_s[\sym i\not\leftarrow] \\ \sst'=\fn{wrap}(\sym h'_c, \sym h'_s, \sym d)}{\ppset{\sst,\sym i,\sst_i}{\sst'}}
\end{mathpar}
\caption{Rules for \code{set} in \SplitPMap}
\label{fig:splitpmap-set-rules}
\end{figure}

We also note that this optimisation, while at times performant, also comes with a cost: checking if a state is concrete also requires traversing the tree (although it doesn't require modifying it), which comes at a cost for larger trees. This could be further optimised, for instance by caching whether larger states are concrete, and invalidating the cache on modification. However, we have not found this cost to be significant enough to warrant this modification. This improvement is thus highly dependent on what code is verified; if there is a majority of symbolic values, then the speed gain is minimal, and it could even be cancelled by the cost of needing to check for concreteness of substates.

\subsection{Abstract Location PMap}

The last, and by far most aggressive optimisation we present is the \emph{abstract location} optimisation. Rather than straightforwardly stating how the optimisation works, we will first take a detour, explaining what abstract locations are.

\subsubsection{Abstract Locations?}

First, we define locations. A location $\loc{x}\in\Loc\subseteq\Val$ is an uninterpreted value, representing a location in memory. It only has a name $x$, such that two locations with different names are always different.

Abstract locations (or ALocs) are, as their name implies, locations lifted to the symbolic realm. An abstract location $\aloc{x}\in\ALoc\subseteq\LVal$ also is characterized by its name, and evaluates to a location: $\forall a\ldotp \exists b\ldotp \expeval{\aloc{a}}=\loc{b}$. Unlike for locations, two syntactically distinct ALocs can still be equal once concretised. Whether they are equal is decided by the engine according to an additional flag of the path condition: the \emph{matching mode}. We denote it $m\in\{\code{MATCH},\code{NO\_MATCH}\}$. When matching is enabled, abstract locations behave similarly to symbolic variables: their equality is decided by the path condition, such that for an empty path condition and $a\neq b$, both $\aloc{a}=\aloc{b}$ and $\aloc{a}\neq\aloc{b}$ are satisfiable. When matching is disabled however, equality of ALocs is only decided by their name: $a=b\Leftrightarrow \aloc{a}=\aloc{b}$. Matching is always enabled for \consume, and always disabled for \produce{} and \execac.

To be able to use abstract locations, the engine must be able to get the ALoc associated to a value; to this end, we introduce the function $\code{to\_aloc}\colon \LVal\rarr\Str^?$. Given a logical value, it \emph{attempts} to find a location or abstract location that it is equal to, and returns its name, if it finds it -- otherwise, it returns $\bot$. It attempts to guess the ALoc by traversing the value's AST, and by looking in the PC for an equality. The crucial point of this is that this is a best effort function, that can fail to find an ALoc the value is associated to. When $\code{to\_aloc}~e=\bot$, the engine is then free to use $\code{fresh\_aloc}$ to generate a fresh new abstract location, and add $e=\aloc{a}$ to the PC, where $a$ is the name of the fresh ALoc. This mechanism is a form of \emph{semantic hash consing}, where the engine looks for any value semantically equal to the expression, and looks for an ALoc associated to it. This approach is sound, as the generated location is fresh, such that even if the value was already bound to a location, it can always be found later that the new location is equal to the old one.

\subsubsection{Abstract Location Optimisation}

We now present the optimisation of \PMap{} using abstract locations: $\ALocPMap(\mmdl)$. Firstly, and unlike the syntactic optimisation or $\SplitPMap$, this optimisation is only applicable to $\PMap(\Loc,\mmdl)$, since abstract locations can only evaluate to locations. The optimisation consists in retrieving the abstract location associated with the given index, and then looking for matches. The strength (and as will be shown later, weakness) of this optimisation is that \emph{there is no branching when matching is disabled}, as indeed the SMT solver is not needed to match the given abstract location with the locations in the heap: syntactic equality is sufficient. Its set of symbolic elements is very similar to \PMap, but with strings as keys (the name of the abstract location): \begin{align*}
	\ALocPMap(\mmdl) &\defeq \Str \finmap \mmdl.\Sigma \times \pset(\LVal)^?
\end{align*}

To take advantage of matching, the definition of \code{get} is modified, adding an argument for the matching mode, $m\in\{\code{MATCH},\code{NO\_MATCH}\}$. Two rules, in their matching and non-matching form, are shown in \autoref{fig:alocpmap-get-rules}. The only difference between the two is the inclusion in the path condition of ${\sym i\notin\{\aloc{a''}:a''\in \dom(\sym h)\}}$, such that the branch is only valid if we're \emph{sure} that the address is not already in the heap. This cannot be included when matching is disabled, because it is too strong a condition: when not matching we already do not attempt to match the abstract location to any of the other indices, so if it results that we skipped a correct branch then the action would vanish (result in no branches), which is not OX sound.

\begin{figure}
	\centering
	\begin{align*}
		\text{with } \sym i\in^? \sym d\defeq \begin{cases}
			\vtrue &\If~\sym d=\bot \\
			\sym i\in \sym d &\Otherwise
		\end{cases}
	\end{align*}
	\begin{mathpar}
\inferrule[ALocPMapGetNoMatchNotFound]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ a = \code{to\_aloc}~\sym i \\ a \neq \bot \\ a\notin\dom(\sym h)}{\ppget{\sst, \sym i, \code{NO\_MATCH}}{\aloc{a}, \bot, [\sym i\in^? \sym d]}}
\and\inferrule[ALocPMapGetMatchNotFound]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ a = \code{to\_aloc}~\sym i \\ a \neq \bot \\ a\notin\dom(\sym h)}{\ppget{\sst, \sym i, \code{MATCH}}{\aloc{a}, \bot, [\sym i \notin \{\aloc{a'}:a'\in \dom(\sym h)\} \land \sym i\in^? \sym d]}}
\and\inferrule[ALocPMapGetNoMatchNew]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ a = \code{to\_aloc}~\sym i \\ a = \bot \\ a'=\code{fresh\_aloc ()}}{\ppget{\sst, \sym i, \code{NO\_MATCH}}{\aloc{a'}, \bot, [\sym i=\aloc{a'} \land \sym i \in^? \sym d]}}
\and\inferrule[ALocPMapGetMatchNew]{(\sym h,\sym d)=\fn{unwrap}(\sst) \\ a = \code{to\_aloc}~\sym i \\ a = \bot \\ a'=\code{fresh\_aloc ()}}{\ppget{\sst, \sym i, \code{MATCH}}{\aloc{a'}, \bot, [\sym i=\aloc{a'} \land \sym i\notin\{\aloc{a''}:a''\in \dom(\sym h)\} \land \sym i \in^? \sym d]}}
	\end{mathpar}
	\caption{Extract of rules for \code{get} in \ALocPMap}
	\label{fig:alocpmap-get-rules}
\end{figure}

This optimisation is particularly powerful in the case where a value that is not known to be equal to any already present location is accessed: for a heap of size $n$, \PMap{} would branch $n+1$ times, whereas \ALocPMap{} does not branch (if not matching), from the rule \ruleref{ALocPMapNoMatchNew}. However, this is also too strong an optimisation. Consider the state model $\PMap(\Loc, \PMap(\Str, \Ex(\Val)))$: the state maps locations to objects, which are string-value mappings.\footnote{This is very similar to the JavaScript state model.} It exposes predicates of the form $\corepred{\exP}{a,n}{x}$, where $a$ is the address in the heap, $n$ is the name of the attribute, and $x$ its value. Now consider the following precondition: \begin{align*}
		\corepred{\exP}{\sym a_1, \strv{val}}{\sym x} * \corepred{\exP}{\sym a_2, \strv{len}}{\sym n}
\end{align*}

When producing this precondition with \PMap, we get two branches: one where ${\sym a_1=\sym a_2}$ and both properties live in the same object, and one where ${\sym a_1\neq \sym a_2}$. However with \ALocPMap{} this only results in one branch: both properties are empty and \emph{the path condition is empty}. In the current definition of symbolic interpretation for \ALocPMap{} (shown in \autoref{fig:alocpmap-satisfiability}), this is not OX sound. Substitutions play a role here, because if at a later point in execution the engine finds $\sym a_1=\sym a_2$ then it may do a substitution and merge the two states, however this is not guaranteed. A solution to this would be to redefine satisfiability of \ALocPMap{}, to take into account the merging of states. Informally, this would mean that a symbolic state models a concrete state if for all concrete addresses, the \emph{composition of all symbolic substates at matching symbolic addresses} is modelled by the concrete substate. For example, the symbolic state $[\sym a_1 \mapsto [\strv{val} \mapsto \sym x], \sym a_2 \mapsto [\strv{len}\mapsto \sym n]]$ would then satisfy a concrete state $[ a \mapsto [\strv{val} \mapsto x, \strv{len}\mapsto n]]$\footnote{We take some liberties when representing the states, for the sake of readability, by omitting the domain sets.} for an empty path condition, since we could compose the two substates together (assuming again that composition is defined for symbolic states, which it isn't in the current formalism).

\begin{figure}
	\centering
	\begin{mathpar}
\inferrule[ALocPMapSymInterpretation]{\forall a\in\dom(\sym h)\ldotp \expeval{\aloc{a}}=i \land i\in\dom(h) \land \theta,s,h(i) \models \sym h(a) \\ \expeval{\{ \aloc{a} : a\in \dom(\sym h)\}}=\dom(h) \\ \expeval{\sym d}=d}{\theta,s,(h,d)\models (\sym h, \sym d)}
	\end{mathpar}
\caption{Symbolic state satisfiability for \ALocPMap}
\label{fig:alocpmap-satisfiability}
\end{figure}

In fact, this unsoundness is also visible when attempting to prove \ref{eq:get-ox-sound}, and another similar issue occurs in the proof for \ref{eq:get-ux-sound}. In both cases, the issue only occurs when the matching mode is \code{NO\_MATCH}, confirming this is the source of the problems.

\subsubsection{In Defense of Abstract Locations}

As shown above, there is currently an unsoundness in optimisations using abstract locations, due to the \emph{matching mode}, and in particular when it is disabled. This occurs because the engine eagerly assumes difference, when it shouldn't always. We argue here that this behaviour, while currently unsound, can be made sound, and has a practical justification.

Consider a simple linear heap, $\PMap(\Loc, \Ex(\Val))$, with the core predicate $\corepred{\exP}{\sym i}{\sym x}$, that we denote $\sym i\mapsto \sym x$. In a heap $\sym i_0\mapsto \sym x_0 * ... * \sym i_n\mapsto \sym x_n$, one's intuition is of course that all locations are different, and indeed both $\PMap(\Loc, \Ex(\Val))$ and $\ALocPMap(\Ex(\Val))$ will create the same state when producing these assertions. In the optimised case, because each index will be new when producing the associated core predicate, $\ALocPMap$ will always generate a new abstract location to associate to it, and consider that it is different from all other locations. This is done \emph{implicitly} -- that is, the path condition is not extended to take this into account. The base version, on the other side, will need to branch for each new assertion. Because these assertions do not already explicitly specify that all addresses are distinct (${\sym i_0 \neq \sym i_1 \land \sym i_0 \neq \sym i_2 \land ...}$), the branch $\sym i_0 = \sym i_1$ is satisfiable; however of course producing $\sym i_1 \mapsto \sym x_1$ into a state $[\sym i_0\mapsto \sym x_0]$ when $\sym i_0 = \sym i_1$ will vanish. This then happens for each address combination; for $n$ addresses, this results in $\frac{n(n+1)}{2}$ inequalities. This makes the PC grow quadratically, slowing down queries sent to the SMT solver, as well as branching and vanishing right after just as many times (see \autoref{fig:pmap-produce-cells}).

\begin{figure}
	\centering
	\includegraphics[width=12cm]{diagrams/pmap-produce-cells.drawio.pdf}
	\caption{Branching when producing cells}
	\label{fig:pmap-produce-cells}
\end{figure}

One could of course specify, along with the assertions, that all addresses are distinct -- this is however quite effortful, and detracts from the already complex task of formulating function specifications.

Furthermore, one can't design a general map state model that makes this assumption -- while for a simple case like $\PMap(\Loc, \Ex(\Val))$ it is always true that different cells must have different addresses (since the cell is exclusively owned), this is not true for more complex state models. For instance take $\PMap(\Loc, \PMap(\Str, \Val))$, where the map contains objects, which are string-value pairs. When describing two objects with non-overlapping keys, if the addresses are not explicitly said to be different then both the branches where they're distinct and where they're separate should be considered. While these two branches should work the same, it results that disabling this abstract location optimisation when producing (i.e. enabling matching on \produce) results in several functions not verifying with Gillian, in WISL, C and JS. It thus seems that this lack of completeness is at least in part caused by developer intuition, where one ``knows'' when writing the specification that the locations should be distinct, without formalising it, and instead relying on this unsound behaviour. This problem seems partly caused by recursive user predicates, where addresses are ``discovered'' as they are unfolded; these locations are, before the unfolding, not named, and as such cannot be stated to be distinct from the others.

This difference between intuition and actual application to a verification setting seems to be caused by ``Barendregt Variable Convention'' \cite{formalbarendregt}, where variables named differently are informally assumed to be different, which can become an issue for an engine that does not properly take it into account. A solution to this would be to better formalise the principles governing abstract locations, for instance using \emph{Nominal Logic} \cite{nominallogic,formalbarendregt}. It provides a sound framework for reasoning about syntactic equivalence of terms within recursive structures, such as recursive user defined predicates.
